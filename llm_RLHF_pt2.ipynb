{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ilRIDbjb2sKi"
      },
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "import torch\n",
        "\n",
        "class ByteTokenizer:\n",
        "    \"\"\"Simple byte-level tokenizer (0..255).\"\"\"\n",
        "    def encode(self, s: str) -> torch.Tensor:\n",
        "        return torch.tensor(list(s.encode('utf-8')), dtype=torch.long)\n",
        "    def decode(self, ids) -> str:\n",
        "        if isinstance(ids, torch.Tensor):\n",
        "            ids = ids.tolist()\n",
        "        return bytes(ids).decode('utf-8', errors='ignore')\n",
        "    @property\n",
        "    def vocab_size(self) -> int:\n",
        "        return 256"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sTBPfDbp-kNc"
      },
      "outputs": [],
      "source": [
        "\n",
        "import torch\n",
        "\n",
        "def top_k_top_p_filtering(logits: torch.Tensor, top_k: int | None = None, top_p: float | None = None):\n",
        "    \"\"\"Filter a distribution of logits using top-k and/or nucleus (top-p) filtering.\n",
        "    - logits: (B, vocab)\n",
        "    Returns filtered logits with -inf for masked entries.\n",
        "    \"\"\"\n",
        "    B, V = logits.shape\n",
        "    filtered = logits.clone()\n",
        "\n",
        "    if top_k is not None and top_k < V:\n",
        "        topk_vals, _ = torch.topk(filtered, top_k, dim=-1)\n",
        "        kth = topk_vals[:, -1].unsqueeze(-1)\n",
        "        filtered[filtered < kth] = float('-inf')\n",
        "\n",
        "    if top_p is not None and 0 < top_p < 1.0:\n",
        "        sorted_logits, sorted_idx = torch.sort(filtered, descending=True, dim=-1)\n",
        "        probs = torch.softmax(sorted_logits, dim=-1)\n",
        "        cumsum = torch.cumsum(probs, dim=-1)\n",
        "        mask = cumsum > top_p\n",
        "        # keep at least 1 token\n",
        "        mask[..., 0] = False\n",
        "        sorted_logits[mask] = float('-inf')\n",
        "        # Scatter back\n",
        "        filtered = torch.full_like(filtered, float('-inf'))\n",
        "        filtered.scatter_(1, sorted_idx, sorted_logits)\n",
        "\n",
        "    return filtered"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "nMzePAIS-nbS"
      },
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "import torch\n",
        "import math\n",
        "\n",
        "class RoPECache:\n",
        "    \"\"\"Precompute cos/sin for positions up to max_pos for even head_dim.\"\"\"\n",
        "    def __init__(self, head_dim: int, max_pos: int, base: float = 10000.0, device: torch.device | None = None):\n",
        "        assert head_dim % 2 == 0, \"RoPE head_dim must be even\"\n",
        "        self.head_dim = head_dim\n",
        "        self.base = base\n",
        "        self.device = device\n",
        "        self._build(max_pos)\n",
        "    def get(self, positions: torch.Tensor):\n",
        "        # positions: (T,) or (1,T)\n",
        "        if positions.dim() == 2:\n",
        "            positions = positions[0]\n",
        "        need = int(positions.max().item()) + 1 if positions.numel() > 0 else 1\n",
        "        if need > self.max_pos:\n",
        "            # grow tables\n",
        "            self._build(max(need, int(self.max_pos * 2)))\n",
        "        cos = self.cos[positions]  # (T, D/2)\n",
        "        sin = self.sin[positions]\n",
        "        return cos, sin\n",
        "\n",
        "    def _build(self, max_pos: int):\n",
        "        \"\"\"(Re)build cos/sin tables for a new max_pos.\"\"\"\n",
        "        self.max_pos = max_pos\n",
        "        inv_freq = 1.0 / (10000.0 ** (torch.arange(0, self.head_dim, 2, device=self.device).float() / self.head_dim))\n",
        "        t = torch.arange(max_pos, device=self.device).float()\n",
        "        freqs = torch.outer(t, inv_freq)  # (max_pos, head_dim/2)\n",
        "        self.cos = torch.cos(freqs)\n",
        "        self.sin = torch.sin(freqs)\n",
        "\n",
        "def apply_rope_single(x: torch.Tensor, cos: torch.Tensor, sin: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"Rotate pairs along last dim for RoPE.\n",
        "    x: (B,H,T,D) with D even; cos/sin: (T,D/2)\n",
        "    \"\"\"\n",
        "    assert x.size(-1) % 2 == 0\n",
        "    cos = cos.unsqueeze(0).unsqueeze(0)  # (1,1,T,D/2)\n",
        "    sin = sin.unsqueeze(0).unsqueeze(0)\n",
        "    x1 = x[..., ::2]\n",
        "    x2 = x[..., 1::2]\n",
        "    xr1 = x1 * cos - x2 * sin\n",
        "    xr2 = x1 * sin + x2 * cos\n",
        "    out = torch.empty_like(x)\n",
        "    out[..., ::2] = xr1\n",
        "    out[..., 1::2] = xr2\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "AfyXDK4B-r7K"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class RMSNorm(nn.Module):\n",
        "    \"\"\"Root Mean Square Layer Normalization.\n",
        "    y = x * g / rms(x),   rms(x) = sqrt(mean(x^2) + eps)\n",
        "    \"\"\"\n",
        "    def __init__(self, dim: int, eps: float = 1e-8):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.weight = nn.Parameter(torch.ones(dim))\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        rms = x.pow(2).mean(dim=-1, keepdim=True).add(self.eps).sqrt()\n",
        "        return (x / rms) * self.weight"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "IwtOdyjt-uNi"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class SwiGLU(nn.Module):\n",
        "    \"\"\"SwiGLU FFN: (xW1) ⊗ swish(xW2) W3  with expansion factor `mult`.\n",
        "    \"\"\"\n",
        "    def __init__(self, dim: int, mult: int = 4, dropout: float = 0.0):\n",
        "        super().__init__()\n",
        "        inner = mult * dim\n",
        "        self.w1 = nn.Linear(dim, inner, bias=False)\n",
        "        self.w2 = nn.Linear(dim, inner, bias=False)\n",
        "        self.w3 = nn.Linear(inner, dim, bias=False)\n",
        "        self.act = nn.SiLU()\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "    def forward(self, x):\n",
        "        a = self.w1(x)\n",
        "        b = self.act(self.w2(x))\n",
        "        return self.drop(self.w3(a * b))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "XGp0h7To-w-i"
      },
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "import torch\n",
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass\n",
        "class KVCache:\n",
        "    k: torch.Tensor  # (B,H,T,D)\n",
        "    v: torch.Tensor  # (B,H,T,D)\n",
        "\n",
        "    @property\n",
        "    def T(self):\n",
        "        return self.k.size(2)\n",
        "\n",
        "class RollingKV:\n",
        "    \"\"\"Rolling buffer with optional attention sink.\n",
        "    Keeps first `sink` tokens + last `window` tokens.\n",
        "    \"\"\"\n",
        "    def __init__(self, window: int, sink: int = 0):\n",
        "        self.window = window\n",
        "        self.sink = sink\n",
        "        self.k = None\n",
        "        self.v = None\n",
        "    def step(self, k_new: torch.Tensor, v_new: torch.Tensor):\n",
        "        if self.k is None:\n",
        "            self.k, self.v = k_new, v_new\n",
        "        else:\n",
        "            self.k = torch.cat([self.k, k_new], dim=2)\n",
        "            self.v = torch.cat([self.v, v_new], dim=2)\n",
        "        # crop\n",
        "        if self.k.size(2) > self.window + self.sink:\n",
        "            sink_part = self.k[:, :, :self.sink, :]\n",
        "            sink_val  = self.v[:, :, :self.sink, :]\n",
        "            tail_k = self.k[:, :, -self.window:, :]\n",
        "            tail_v = self.v[:, :, -self.window:, :]\n",
        "            self.k = torch.cat([sink_part, tail_k], dim=2)\n",
        "            self.v = torch.cat([sink_val, tail_v], dim=2)\n",
        "        return self.k, self.v"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "tEq_2Ut4_FKa"
      },
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "import math, torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class CausalSelfAttentionModern(nn.Module):\n",
        "    def __init__(self, n_embd: int, n_head: int, dropout: float = 0.0,\n",
        "                 rope: bool = True, max_pos: int = 4096,\n",
        "                 sliding_window: int | None = None, attention_sink: int = 0,\n",
        "                 n_kv_head: int | None = None):  # ← NEW\n",
        "        super().__init__()\n",
        "        assert n_embd % n_head == 0, \"n_embd must be divisible by n_head\"\n",
        "        self.n_head = n_head\n",
        "        self.n_kv_head = n_kv_head or n_head      # ← NEW (GQA defaults to MHA)\n",
        "        assert self.n_head % self.n_kv_head == 0, \"n_head must be multiple of n_kv_head (GQA grouping)\"\n",
        "        self.group_size = self.n_head // self.n_kv_head\n",
        "        self.d_head = n_embd // n_head\n",
        "\n",
        "        # Separate projections for Q vs K/V (sizes differ under GQA)  ← CHANGED\n",
        "        self.wq  = nn.Linear(n_embd, self.n_head   * self.d_head, bias=False)\n",
        "        self.wk  = nn.Linear(n_embd, self.n_kv_head * self.d_head, bias=False)\n",
        "        self.wv  = nn.Linear(n_embd, self.n_kv_head * self.d_head, bias=False)\n",
        "        self.proj = nn.Linear(n_embd, n_embd, bias=False)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.use_rope = rope\n",
        "        self.rope_cache: RoPECache | None = None\n",
        "        self.max_pos = max_pos\n",
        "        self.sliding_window = sliding_window\n",
        "        self.attention_sink = attention_sink\n",
        "\n",
        "    def _maybe_init_rope(self, device):\n",
        "        if self.use_rope and self.rope_cache is None:\n",
        "            self.rope_cache = RoPECache(self.d_head, self.max_pos, device=device)\n",
        "\n",
        "    def forward(self, x: torch.Tensor, kv_cache: KVCache | None = None, start_pos: int = 0):\n",
        "        \"\"\"x: (B,T,C). If kv_cache given, we assume generation (T small, often 1).\"\"\"\n",
        "        B, T, C = x.shape\n",
        "        self._maybe_init_rope(x.device)\n",
        "\n",
        "        # Projections\n",
        "        q = self.wq(x).view(B, T, self.n_head,   self.d_head).transpose(1, 2)    # (B,H, T,D)\n",
        "        k = self.wk(x).view(B, T, self.n_kv_head, self.d_head).transpose(1, 2)   # (B,Hk,T,D)\n",
        "        v = self.wv(x).view(B, T, self.n_kv_head, self.d_head).transpose(1, 2)   # (B,Hk,T,D)\n",
        "\n",
        "        # RoPE on *current* tokens (cached keys are already rotated)\n",
        "        if self.use_rope:\n",
        "            pos = torch.arange(start_pos, start_pos + T, device=x.device)\n",
        "            cos, sin = self.rope_cache.get(pos)\n",
        "            q = apply_rope_single(q, cos, sin)   # (B,H, T,D)\n",
        "            k = apply_rope_single(k, cos, sin)   # (B,Hk,T,D)\n",
        "\n",
        "        # Concatenate past cache (cache is stored in Hk heads)\n",
        "        if kv_cache is not None:\n",
        "            k_all = torch.cat([kv_cache.k, k], dim=2)  # (B,Hk, Tpast+T, D)\n",
        "            v_all = torch.cat([kv_cache.v, v], dim=2)\n",
        "        else:\n",
        "            k_all, v_all = k, v\n",
        "\n",
        "        # Sliding-window + attention-sink (crop along seq length)\n",
        "        if self.sliding_window is not None and k_all.size(2) > (self.sliding_window + self.attention_sink):\n",
        "            s = self.attention_sink\n",
        "            k_all = torch.cat([k_all[:, :, :s, :], k_all[:, :, -self.sliding_window:, :]], dim=2)\n",
        "            v_all = torch.cat([v_all[:, :, :s, :], v_all[:, :, -self.sliding_window:, :]], dim=2)\n",
        "\n",
        "        # --- GQA expand: repeat K/V heads to match Q heads before attention ---\n",
        "        if self.n_kv_head != self.n_head:\n",
        "            k_attn = k_all.repeat_interleave(self.group_size, dim=1)  # (B,H,Tk,D)\n",
        "            v_attn = v_all.repeat_interleave(self.group_size, dim=1)  # (B,H,Tk,D)\n",
        "        else:\n",
        "            k_attn, v_attn = k_all, v_all\n",
        "\n",
        "        # Scaled dot-product attention (PyTorch scales internally)\n",
        "        is_causal = kv_cache is None\n",
        "        y = F.scaled_dot_product_attention(q, k_attn, v_attn,\n",
        "                                           attn_mask=None,\n",
        "                                           dropout_p=self.dropout.p if self.training else 0.0,\n",
        "                                           is_causal=is_causal)          # (B,H,T,D)\n",
        "\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
        "        y = self.proj(y)\n",
        "\n",
        "        # Update KV cache (store compact Hk heads, not expanded)\n",
        "        if kv_cache is not None:\n",
        "            k_new = torch.cat([kv_cache.k, k], dim=2)  # (B,Hk,*,D)\n",
        "            v_new = torch.cat([kv_cache.v, v], dim=2)\n",
        "        else:\n",
        "            k_new, v_new = k, v\n",
        "        new_cache = KVCache(k_new, v_new)\n",
        "        return y, new_cache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "5UByeoDg-2kR"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "# from rmsnorm import RMSNorm\n",
        "# from swiglu import SwiGLU\n",
        "# from attn_modern import CausalSelfAttentionModern\n",
        "\n",
        "class TransformerBlockModern(nn.Module):\n",
        "    def __init__(self, n_embd: int, n_head: int, dropout: float = 0.0,\n",
        "                 use_rmsnorm: bool = True, use_swiglu: bool = True,\n",
        "                 rope: bool = True, max_pos: int = 4096,\n",
        "                 sliding_window: int | None = None, attention_sink: int = 0, n_kv_head: int | None = None):\n",
        "        super().__init__()\n",
        "        Norm = RMSNorm if use_rmsnorm else nn.LayerNorm\n",
        "        self.ln1 = Norm(n_embd)\n",
        "        self.attn = CausalSelfAttentionModern(n_embd, n_head, dropout, rope, max_pos, sliding_window, attention_sink, n_kv_head)\n",
        "        self.ln2 = Norm(n_embd)\n",
        "        self.ffn = SwiGLU(n_embd, mult=4, dropout=dropout) if use_swiglu else nn.Sequential(\n",
        "            nn.Linear(n_embd, 4*n_embd), nn.GELU(), nn.Linear(4*n_embd, n_embd), nn.Dropout(dropout)\n",
        "        )\n",
        "    def forward(self, x, kv_cache=None, start_pos: int = 0):\n",
        "        a, kv_cache = self.attn(self.ln1(x), kv_cache=kv_cache, start_pos=start_pos)\n",
        "        x = x + a\n",
        "        x = x + self.ffn(self.ln2(x))\n",
        "        return x, kv_cache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "0jk6I_Gq_riZ"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class TransformerBlockModern(nn.Module):\n",
        "    def __init__(self, n_embd: int, n_head: int, dropout: float = 0.0,\n",
        "                 use_rmsnorm: bool = True, use_swiglu: bool = True,\n",
        "                 rope: bool = True, max_pos: int = 4096,\n",
        "                 sliding_window: int | None = None, attention_sink: int = 0, n_kv_head: int | None = None):\n",
        "        super().__init__()\n",
        "        Norm = RMSNorm if use_rmsnorm else nn.LayerNorm\n",
        "        self.ln1 = Norm(n_embd)\n",
        "        self.attn = CausalSelfAttentionModern(n_embd, n_head, dropout, rope, max_pos, sliding_window, attention_sink, n_kv_head)\n",
        "        self.ln2 = Norm(n_embd)\n",
        "        self.ffn = SwiGLU(n_embd, mult=4, dropout=dropout) if use_swiglu else nn.Sequential(\n",
        "            nn.Linear(n_embd, 4*n_embd), nn.GELU(), nn.Linear(4*n_embd, n_embd), nn.Dropout(dropout)\n",
        "        )\n",
        "    def forward(self, x, kv_cache=None, start_pos: int = 0):\n",
        "        a, kv_cache = self.attn(self.ln1(x), kv_cache=kv_cache, start_pos=start_pos)\n",
        "        x = x + a\n",
        "        x = x + self.ffn(self.ln2(x))\n",
        "        return x, kv_cache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "yI7dvzZi-8rB"
      },
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "# from block_modern import TransformerBlockModern\n",
        "# from tokenizer import ByteTokenizer\n",
        "\n",
        "# Get the absolute path to the folder that contains part_2 and part_3\n",
        "import os, sys\n",
        "# parent_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))\n",
        "# sys.path.insert(0, parent_dir)\n",
        "\n",
        "class GPTModern(nn.Module):\n",
        "    def __init__(self, vocab_size: int = 256, block_size: int = 256,\n",
        "                 n_layer: int=4, n_head: int=4, n_embd: int=256, dropout: float=0.0,\n",
        "                 use_rmsnorm: bool = True, use_swiglu: bool = True, rope: bool = True,\n",
        "                 max_pos: int = 4096, sliding_window: int | None = None, attention_sink: int = 0, n_kv_head: int | None = None):\n",
        "        super().__init__()\n",
        "        self.block_size = block_size\n",
        "        self.tok_emb = nn.Embedding(vocab_size, n_embd)\n",
        "        # self.pos_emb = nn.Embedding(block_size, n_embd)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.blocks = nn.ModuleList([\n",
        "            TransformerBlockModern(n_embd, n_head, dropout, use_rmsnorm, use_swiglu, rope, max_pos, sliding_window, attention_sink, n_kv_head)\n",
        "            for _ in range(n_layer)\n",
        "        ])\n",
        "        self.ln_f = nn.Identity() if use_rmsnorm else nn.LayerNorm(n_embd)\n",
        "        self.head = nn.Linear(n_embd, vocab_size, bias=False)\n",
        "\n",
        "    def forward(self, idx: torch.Tensor, targets: torch.Tensor | None = None, kv_cache_list=None, start_pos: int = 0):\n",
        "        B, T = idx.shape\n",
        "        assert T <= self.block_size\n",
        "        pos = torch.arange(0, T, device=idx.device).unsqueeze(0)\n",
        "        x = self.tok_emb(idx)\n",
        "        # + self.pos_emb(pos)\n",
        "        x = self.drop(x)\n",
        "\n",
        "        new_caches = []\n",
        "        for i, blk in enumerate(self.blocks):\n",
        "            cache = None if kv_cache_list is None else kv_cache_list[i]\n",
        "            x, cache = blk(x, kv_cache=cache, start_pos=start_pos)\n",
        "            new_caches.append(cache)\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.head(x)\n",
        "\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            import torch.nn.functional as F\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
        "        return logits, loss, new_caches\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self,\n",
        "                 prompt: torch.Tensor,\n",
        "                 max_new_tokens=200,\n",
        "                 temperature=1.0,\n",
        "                 top_k=50,\n",
        "                 top_p=None,\n",
        "                 eos_id=1, # addition from part 6 for early stopping\n",
        "                 sliding_window: int | None = None,\n",
        "                 attention_sink: int = 0):\n",
        "        # try:\n",
        "        #     from utils import top_k_top_p_filtering as _tk'\n",
        "        # except Exception:\n",
        "        #     _tk = lambda x, **_: x\n",
        "\n",
        "        self.eval()\n",
        "        idx = prompt\n",
        "        kvs = [None] * len(self.blocks)\n",
        "\n",
        "        for _ in range(max_new_tokens):\n",
        "            # feed full prompt once; then only the last token\n",
        "            idx_cond = idx[:, -self.block_size:] if kvs[0] is None else idx[:, -1:]\n",
        "\n",
        "            # absolute start position from cache length (0 on first step)\n",
        "            start_pos = 0 if kvs[0] is None else kvs[0].k.size(2)\n",
        "\n",
        "            logits, _, kvs = self(idx_cond, kv_cache_list=kvs, start_pos=start_pos)\n",
        "\n",
        "            next_logits = logits[:, -1, :] / max(temperature, 1e-6)\n",
        "            next_logits = top_k_top_p_filtering(next_logits, top_k=top_k, top_p=top_p)\n",
        "            probs = torch.softmax(next_logits, dim=-1)\n",
        "            next_id = torch.argmax(probs, dim=-1, keepdim=True) if temperature == 0.0 else torch.multinomial(probs, 1)\n",
        "            idx = torch.cat([idx, next_id], dim=1)\n",
        "\n",
        "            # addition from part 6 for early stopping\n",
        "            if eos_id is not None:\n",
        "                if (next_id == eos_id).all():\n",
        "                    break\n",
        "\n",
        "        return idx\n",
        "\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate_nocache(self, prompt: torch.Tensor, max_new_tokens=200, temperature=1.0, top_k=50, top_p=None,\n",
        "                sliding_window: int | None = None, attention_sink: int = 0):\n",
        "        # try:\n",
        "        #     print('from utils import top_k_top_p_filtering as _tk')\n",
        "        # except Exception:\n",
        "        #     _tk = lambda x, **_: x\n",
        "\n",
        "        self.eval()\n",
        "        idx = prompt\n",
        "\n",
        "        for _ in range(max_new_tokens):\n",
        "            # always run a full forward over the cropped window, with NO cache\n",
        "            idx_cond = idx[:, -self.block_size:]\n",
        "            # absolute position of first token in the window (matches cached path)\n",
        "            start_pos = idx.size(1) - idx_cond.size(1)\n",
        "\n",
        "            logits, _, _ = self(idx_cond, kv_cache_list=None, start_pos=start_pos)\n",
        "\n",
        "            next_logits = logits[:, -1, :] / max(temperature, 1e-6)\n",
        "            next_logits = top_k_top_p_filtering(next_logits, top_k=top_k, top_p=top_p)\n",
        "            probs = torch.softmax(next_logits, dim=-1)\n",
        "            topv, topi = torch.topk(probs, 10)\n",
        "            print(\"top ids:\", topi.tolist())\n",
        "            print(\"top vs:\", topv.tolist())\n",
        "            next_id = torch.argmax(probs, dim=-1, keepdim=True) if temperature == 0.0 else torch.multinomial(probs, 1)\n",
        "            idx = torch.cat([idx, next_id], dim=1)\n",
        "\n",
        "        return idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MG9McWYI_Mp5",
        "outputId": "a3e827ed-4c11-4e80-93ea-b7eb5fc3bcaf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated 200 tokens in 0.40 sec\n",
            "top ids: [[231, 175, 176, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[255, 175, 161, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[34, 167, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[210, 168, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[118, 168, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[201, 168, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[8, 175, 177, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[54, 168, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[235, 168, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[71, 167, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[159, 167, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[104, 175, 176, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[3, 167, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[174, 168, 175, 173, 172, 171, 170, 169, 162, 163]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[92, 167, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[109, 168, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[123, 167, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[203, 168, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[58, 167, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[99, 167, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[2, 167, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[49, 168, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[233, 168, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[52, 168, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[166, 168, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[126, 167, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[243, 168, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[226, 167, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[101, 167, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[94, 167, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[180, 168, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[103, 167, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[186, 167, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[84, 168, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[140, 168, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[224, 175, 176, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[211, 168, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[243, 168, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[226, 167, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[101, 167, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[94, 167, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[180, 168, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[103, 167, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[186, 167, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[84, 168, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[140, 168, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[224, 175, 176, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[211, 168, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[243, 168, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[226, 167, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[101, 167, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[94, 167, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[180, 168, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[103, 167, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[186, 167, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[84, 168, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[140, 168, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[224, 175, 176, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[211, 168, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[243, 168, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[226, 167, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[101, 167, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[94, 167, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[180, 168, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[103, 167, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[186, 167, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[84, 168, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[140, 168, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[224, 175, 176, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[211, 168, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[243, 168, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[226, 167, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[101, 167, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[94, 167, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[180, 168, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[103, 167, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[186, 167, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[84, 168, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[140, 168, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[224, 175, 176, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[211, 168, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[243, 168, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[226, 167, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[101, 167, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[115, 168, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[88, 168, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[21, 168, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[118, 168, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[201, 168, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[17, 168, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[123, 167, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[203, 168, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[58, 167, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[99, 167, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[2, 167, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[65, 175, 161, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[154, 167, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[24, 168, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[143, 168, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[104, 175, 176, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[97, 175, 176, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[41, 168, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[247, 168, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[191, 175, 161, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[84, 168, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[140, 168, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[224, 175, 176, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[211, 168, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[243, 168, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[226, 167, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[101, 167, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[94, 167, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[180, 168, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[103, 167, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[186, 167, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[84, 168, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[140, 168, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[224, 175, 176, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[172, 168, 175, 174, 173, 171, 170, 169, 162, 163]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[157, 167, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[190, 167, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[37, 167, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[91, 167, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[113, 168, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[176, 168, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[114, 168, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[194, 167, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[255, 175, 161, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[34, 167, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[37, 167, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[91, 167, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[113, 168, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[176, 168, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[114, 168, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[194, 167, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[255, 175, 161, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[34, 167, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[37, 167, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[91, 167, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[28, 167, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[24, 168, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[143, 168, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[104, 175, 176, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[3, 167, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[174, 168, 175, 173, 172, 171, 170, 169, 162, 163]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[92, 167, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[109, 168, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[123, 167, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[203, 168, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[58, 167, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[99, 167, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[2, 167, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[65, 175, 161, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[254, 167, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[194, 167, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[255, 175, 161, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[34, 167, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[37, 167, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[91, 167, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[28, 167, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[24, 168, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[143, 168, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[104, 175, 176, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[3, 167, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[174, 168, 175, 173, 172, 171, 170, 169, 162, 163]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[92, 167, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[109, 168, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[123, 167, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[203, 168, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[58, 167, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[99, 167, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[2, 167, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[65, 175, 161, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[154, 167, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[24, 168, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[143, 168, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[104, 175, 176, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[3, 167, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[174, 168, 175, 173, 172, 171, 170, 169, 162, 163]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[92, 167, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[109, 168, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[123, 167, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[203, 168, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[58, 167, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[99, 167, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[2, 167, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[49, 168, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[233, 168, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[52, 168, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[166, 168, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[126, 167, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[243, 168, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[226, 167, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[101, 167, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[94, 167, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[180, 168, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[103, 167, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[186, 167, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[84, 168, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "top ids: [[140, 168, 175, 174, 173, 172, 171, 170, 169, 162]]\n",
            "top vs: [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]\n",
            "(nocache) Generated 200 tokens in 0.96 sec\n",
            "\n",
            "\"v\b6Gh\u0003\\m{:c\u000214~e^gTe^gTe^gTe^gTe^gTe^gT\u000fm{:c\u0002A\u0018ha0Te^gTଝ\u0015v\u0011{:c\u0002A\u0018ha0zFn\"%[qr\"%[qr\"%[\u001c\u0018h\u0003\\m{:c\u0002\u0015vHsX\u0015vHsX\u0015vHsX\u0015v\n",
            "\n",
            "\"v\b6Gh\u0003\\m{:c\u000214~e^gTe^gTe^gTe^gTe^gTesX\u0015v\u0011{:c\u0002A\u0018ha)Te^gTଝ%[qr\"%[qr\"%[\u001c\u0018h\u0003\\m{:c\u0002A\"%[\u001c\u0018h\u0003\\m{:c\u0002A\u0018h\u0003\\m{:c\u000214~e^gT\n"
          ]
        }
      ],
      "source": [
        "import argparse, torch\n",
        "import argparse, torch, sys\n",
        "\n",
        "import time\n",
        "\n",
        "\n",
        "def main(argv:None):\n",
        "    if argv is None:\n",
        "        argv = sys.argv[1:]\n",
        "\n",
        "    p = argparse.ArgumentParser()\n",
        "    p.add_argument('--rmsnorm', action='store_true')\n",
        "    p.add_argument('--rope', action='store_true')\n",
        "    p.add_argument('--swiglu', action='store_true')\n",
        "    p.add_argument('--sliding_window', type=int, default=None)\n",
        "    p.add_argument('--sink', type=int, default=0)\n",
        "    p.add_argument('--group_size', type=int, default=2)\n",
        "    p.add_argument('--tokens', type=int, default=120)\n",
        "    p.add_argument('--cpu', action='store_true')\n",
        "    args = p.parse_args(argv)\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() and not args.cpu else 'cpu')\n",
        "\n",
        "    tok = ByteTokenizer()\n",
        "    model = GPTModern(vocab_size=tok.vocab_size, block_size=128, n_layer=2, n_head=4, n_embd=128,\n",
        "                      use_rmsnorm=args.rmsnorm, use_swiglu=args.swiglu, rope=args.rope,\n",
        "                      max_pos=4096, sliding_window=args.sliding_window, attention_sink=args.sink, n_kv_head=args.group_size).to(device)\n",
        "\n",
        "    # empty prompt → newline\n",
        "    prompt = torch.tensor([[10]], dtype=torch.long, device=device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        start = time.time()\n",
        "        out = model.generate(prompt, max_new_tokens=args.tokens, temperature=0.0, top_k=50, top_p=None,\n",
        "                              sliding_window=args.sliding_window, attention_sink=args.sink)\n",
        "        print(f\"Generated {args.tokens} tokens in {time.time()-start:.2f} sec\")\n",
        "\n",
        "        start = time.time()\n",
        "        out_nocache = model.generate_nocache(prompt, max_new_tokens=args.tokens, temperature=0.0, top_k=50, top_p=None,\n",
        "                              sliding_window=args.sliding_window, attention_sink=args.sink)\n",
        "        print(f\"(nocache) Generated {args.tokens} tokens in {time.time()-start:.2f} sec\")\n",
        "    print(tok.decode(out[0].cpu()))\n",
        "    print(tok.decode(out_nocache[0].cpu()))\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  main([\n",
        "      '--rmsnorm', '--rope', '--swiglu', '--sliding_window', '64', '--sink', '4', '--tokens', '200'\n",
        "  ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ba9VewUh_-f_"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
