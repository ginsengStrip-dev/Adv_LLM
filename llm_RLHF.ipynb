{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "06ZO2LmIjRdl"
      },
      "outputs": [],
      "source": [
        "# import math\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torch.nn.functional as F\n",
        "\n",
        "\n",
        "# def causal_mask(T: int, device=None):\n",
        "#     \"\"\"Returns a bool mask where True means *masked* (disallowed).\n",
        "#     Shape: (1, 1, T, T) suitable for broadcasting with (B, heads, T, T).\n",
        "#     \"\"\"\n",
        "#     m = torch.triu(torch.ones((T, T), dtype=torch.bool, device=device), diagonal=1)\n",
        "\n",
        "#     return m.view(1, 1, T, T)\n",
        "\n",
        "# \"\"\"1.1 Positional encodings (absolute learned + sinusoidal).\"\"\"\n",
        "\n",
        "\n",
        "# class LearnedPositionalEncoding(nn.Module):\n",
        "#     def __init__(self, max_len: int, d_model: int):\n",
        "#         super().__init__()\n",
        "#         self.emb = nn.Embedding(max_len, d_model)\n",
        "\n",
        "#     def forward(self, x: torch.Tensor):\n",
        "#         # x: (B, T, d_model) — we only need its T and device\n",
        "#         B, T, _ = x.shape\n",
        "#         pos = torch.arange(T, device=x.device)\n",
        "#         pos_emb = self.emb(pos)  # (T, d_model)\n",
        "#         return x + pos_emb.unsqueeze(0)  # broadcast over batch\n",
        "\n",
        "# class SinusoidalPositionalEncoding(nn.Module):\n",
        "#     def __init__(self, max_len: int, d_model: int):\n",
        "#         super().__init__()\n",
        "#         pe = torch.zeros(max_len, d_model)\n",
        "#         position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "#         div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "#         pe[:, 0::2] = torch.sin(position * div_term)\n",
        "#         pe[:, 1::2] = torch.cos(position * div_term)\n",
        "#         self.register_buffer('pe', pe)  # (max_len, d_model)\n",
        "\n",
        "#     def forward(self, x: torch.Tensor):\n",
        "#         B, T, _ = x.shape\n",
        "#         return x + self.pe[:T].unsqueeze(0)\n",
        "\n",
        "\n",
        "# class SingleHeadSelfAttention(nn.Module):\n",
        "#     \"\"\"1.3 Single-head attention (explicit shapes).\"\"\"\n",
        "#     def __init__(self, d_model: int, d_k: int, dropout: float = 0.0, trace_shapes: bool = False):\n",
        "#         super().__init__()\n",
        "#         self.q = nn.Linear(d_model, d_k, bias=False)\n",
        "#         self.k = nn.Linear(d_model, d_k, bias=False)\n",
        "#         self.v = nn.Linear(d_model, d_k, bias=False)\n",
        "#         self.dropout = nn.Dropout(dropout)\n",
        "#         self.trace_shapes = trace_shapes\n",
        "\n",
        "#     def forward(self, x: torch.Tensor):  # x: (B, T, d_model)\n",
        "#         B, T, _ = x.shape\n",
        "#         q = self.q(x)  # (B,T,d_k)\n",
        "#         k = self.k(x)  # (B,T,d_k)\n",
        "#         v = self.v(x)  # (B,T,d_k)\n",
        "#         if self.trace_shapes:\n",
        "#             print(f\"q {q.shape}  k {k.shape}  v {v.shape}\")\n",
        "#         scale = 1.0 / math.sqrt(q.size(-1))\n",
        "#         attn = torch.matmul(q, k.transpose(-2, -1)) * scale  # (B,T,T)\n",
        "#         mask = causal_mask(T, device=x.device)\n",
        "#         attn = attn.masked_fill(mask.squeeze(1), float('-inf'))\n",
        "#         w = F.softmax(attn, dim=-1)\n",
        "#         w = self.dropout(w)\n",
        "#         out = torch.matmul(w, v)  # (B,T,d_k)\n",
        "#         if self.trace_shapes:\n",
        "#             print(f\"weights {w.shape}  out {out.shape}\")\n",
        "#         return out, w\n",
        "#   # from attn_mask import causal_mask\n",
        "\n",
        "# class MultiHeadSelfAttention(nn.Module):\n",
        "#     \"\"\"1.4 Multi-head attention with explicit shape tracing.\n",
        "\n",
        "#     Dimensions (before masking):\n",
        "#       x:      (B, T, d_model)\n",
        "#       qkv:    (B, T, 3*d_model)\n",
        "#       view→   (B, T, 3, n_head, d_head)   where d_head = d_model // n_head\n",
        "#       split→  q,k,v each (B, T, n_head, d_head)\n",
        "#       swap→   (B, n_head, T, d_head)\n",
        "#       scores: (B, n_head, T, T) = q @ k^T / sqrt(d_head)\n",
        "#       weights:(B, n_head, T, T) = softmax(scores)\n",
        "#       ctx:    (B, n_head, T, d_head) = weights @ v\n",
        "#       merge:  (B, T, n_head*d_head) = (B, T, d_model)\n",
        "#     \"\"\"\n",
        "#     def __init__(self, d_model: int, n_head: int, dropout: float = 0.0, trace_shapes: bool = True):\n",
        "#         super().__init__()\n",
        "#         assert d_model % n_head == 0, \"d_model must be divisible by n_head\"\n",
        "#         self.n_head = n_head\n",
        "#         self.d_head = d_model // n_head\n",
        "#         self.qkv = nn.Linear(d_model, 3 * d_model, bias=False)\n",
        "#         self.proj = nn.Linear(d_model, d_model, bias=False)\n",
        "#         self.dropout = nn.Dropout(dropout)\n",
        "#         self.trace_shapes = trace_shapes\n",
        "\n",
        "#     def forward(self, x: torch.Tensor):  # (B,T,d_model)\n",
        "#         B, T, C = x.shape\n",
        "#         qkv = self.qkv(x)                          # (B,T,3*C)\n",
        "#         qkv = qkv.view(B, T, 3, self.n_head, self.d_head)  # (B,T,3,heads,dim)\n",
        "#         if self.trace_shapes:\n",
        "#             print(\"qkv view:\", qkv.shape)\n",
        "#         q, k, v = qkv.unbind(dim=2)               # each: (B,T,heads,dim)\n",
        "#         q = q.transpose(1, 2)                      # (B,heads,T,dim)\n",
        "#         k = k.transpose(1, 2)\n",
        "#         v = v.transpose(1, 2)\n",
        "#         if self.trace_shapes:\n",
        "#             print(\"q:\", q.shape, \"k:\", k.shape, \"v:\", v.shape)\n",
        "\n",
        "#         scale = 1.0 / math.sqrt(self.d_head)\n",
        "#         attn = torch.matmul(q, k.transpose(-2, -1)) * scale  # (B,heads,T,T)\n",
        "#         mask = causal_mask(T, device=x.device)\n",
        "#         attn = attn.masked_fill(mask, float('-inf'))\n",
        "#         w = F.softmax(attn, dim=-1)\n",
        "#         w = self.dropout(w)\n",
        "#         ctx = torch.matmul(w, v)                  # (B,heads,T,dim)\n",
        "#         if self.trace_shapes:\n",
        "#             print(\"weights:\", w.shape, \"ctx:\", ctx.shape)\n",
        "#         out = ctx.transpose(1, 2).contiguous().view(B, T, C)  # (B,T,d_model)\n",
        "#         out = self.proj(out)\n",
        "#         if self.trace_shapes:\n",
        "#             print(\"out:\", out.shape)\n",
        "#         return out, w\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# class FeedForward(nn.Module):\n",
        "#     \"\"\"1.5 FFN with expansion factor `mult`.\n",
        "\n",
        "#     Dimensions:\n",
        "#       input:     (B, T, d_model)\n",
        "#       inner:     (B, T, mult*d_model)\n",
        "#       output:    (B, T, d_model)\n",
        "\n",
        "#     `mult*d_model` means the hidden width is `mult` times larger than `d_model`.\n",
        "#     Typical values: mult=4 for GELU FFN in GPT-style blocks.\n",
        "#     \"\"\"\n",
        "#     def __init__(self, d_model: int, mult: int = 4, dropout: float = 0.0):\n",
        "#         super().__init__()\n",
        "#         self.net = nn.Sequential(\n",
        "#             nn.Linear(d_model, mult * d_model),\n",
        "#             nn.GELU(),\n",
        "#             nn.Linear(mult * d_model, d_model),\n",
        "#             nn.Dropout(dropout),\n",
        "#         )\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         return self.net(x)\n",
        "\n",
        "\n",
        "# class TransformerBlock(nn.Module):\n",
        "#     \"\"\"1.6 Transformer block = LN → MHA → residual → LN → FFN → residual.\"\"\"\n",
        "#     def __init__(self, d_model: int, n_head: int, dropout: float = 0.0):\n",
        "#         super().__init__()\n",
        "#         self.ln1 = nn.LayerNorm(d_model)\n",
        "#         self.attn = MultiHeadSelfAttention(d_model, n_head, dropout)\n",
        "#         self.ln2 = nn.LayerNorm(d_model)\n",
        "#         self.ffn = FeedForward(d_model, mult=4, dropout=dropout)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = x + self.attn(self.ln1(x))[0]\n",
        "#         x = x + self.ffn(self.ln2(x))\n",
        "#         return x\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import annotations\n",
        "import torch\n",
        "\n",
        "class ByteTokenizer:\n",
        "    \"\"\"Ultra-simple byte-level tokenizer.\n",
        "    - encode(str) -> LongTensor [N]\n",
        "    - decode(Tensor[int]) -> str\n",
        "    - vocab_size = 256\n",
        "    \"\"\"\n",
        "    def encode(self, s: str) -> torch.Tensor:\n",
        "        return torch.tensor(list(s.encode('utf-8')), dtype=torch.long)\n",
        "\n",
        "    def decode(self, ids) -> str:\n",
        "        if isinstance(ids, torch.Tensor):\n",
        "            ids = ids.tolist()\n",
        "        return bytes(ids).decode('utf-8', errors='ignore')\n",
        "\n",
        "    @property\n",
        "    def vocab_size(self) -> int:\n",
        "        return 256"
      ],
      "metadata": {
        "id": "C85WyV4gpH_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import annotations\n",
        "from pathlib import Path\n",
        "import torch\n",
        "\n",
        "class ByteDataset:\n",
        "    \"\"\"Holds raw bytes of a text file and yields (x,y) blocks for LM.\n",
        "    - block_size: sequence length (context window)\n",
        "    - split: fraction for training (rest is val)\n",
        "    \"\"\"\n",
        "    def __init__(self, path: str, block_size: int = 256, split: float = 0.9):\n",
        "        data = Path(path).read_bytes()\n",
        "        data = torch.tensor(list(data), dtype=torch.long)\n",
        "        n = int(len(data) * split)\n",
        "        self.train = data[:n]\n",
        "        self.val = data[n:]\n",
        "        self.block_size = block_size\n",
        "\n",
        "    def get_batch(self, which: str, batch_size: int, device: torch.device):\n",
        "        buf = self.train if which == 'train' else self.val\n",
        "        assert len(buf) > self.block_size + 1, 'file too small for given block_size'\n",
        "        ix = torch.randint(0, len(buf) - self.block_size - 1, (batch_size,))\n",
        "        x = torch.stack([buf[i:i+self.block_size] for i in ix])\n",
        "        y = torch.stack([buf[i+1:i+1+self.block_size] for i in ix])\n",
        "        return x.to(device), y.to(device)"
      ],
      "metadata": {
        "id": "3gJnl_BNqrww"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import annotations\n",
        "import torch\n",
        "\n",
        "def top_k_top_p_filtering(logits: torch.Tensor, top_k: int | None = None, top_p: float | None = None):\n",
        "    \"\"\"Filter a distribution of logits using top-k and/or nucleus (top-p) filtering.\n",
        "    - logits: (B, vocab)\n",
        "    Returns filtered logits with -inf for masked entries.\n",
        "    \"\"\"\n",
        "    B, V = logits.shape\n",
        "    filtered = logits.clone()\n",
        "\n",
        "    if top_k is not None and top_k < V:\n",
        "        topk_vals, _ = torch.topk(filtered, top_k, dim=-1)\n",
        "        kth = topk_vals[:, -1].unsqueeze(-1)\n",
        "        filtered[filtered < kth] = float('-inf')\n",
        "\n",
        "    if top_p is not None and 0 < top_p < 1.0:\n",
        "        sorted_logits, sorted_idx = torch.sort(filtered, descending=True, dim=-1)\n",
        "        probs = torch.softmax(sorted_logits, dim=-1)\n",
        "        cumsum = torch.cumsum(probs, dim=-1)\n",
        "        mask = cumsum > top_p\n",
        "        # keep at least 1 token\n",
        "        mask[..., 0] = False\n",
        "        sorted_logits[mask] = float('-inf')\n",
        "        # Scatter back\n",
        "        filtered = torch.full_like(filtered, float('-inf'))\n",
        "        filtered.scatter_(1, sorted_idx, sorted_logits)\n",
        "\n",
        "    return filtered"
      ],
      "metadata": {
        "id": "M5AWUGh6szxW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import annotations\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# ---- Blocks (self-contained for isolation) ----\n",
        "class CausalSelfAttention(nn.Module):\n",
        "    def __init__(self, n_embd: int, n_head: int, dropout: float = 0.0):\n",
        "        super().__init__()\n",
        "        assert n_embd % n_head == 0\n",
        "        self.n_head = n_head\n",
        "        self.d_head = n_embd // n_head\n",
        "        self.qkv = nn.Linear(n_embd, 3 * n_embd, bias=False)\n",
        "        self.proj = nn.Linear(n_embd, n_embd, bias=False)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x: torch.Tensor):  # (B,T,C)\n",
        "        B, T, C = x.shape\n",
        "        qkv = self.qkv(x).view(B, T, 3, self.n_head, self.d_head)\n",
        "        q, k, v = qkv.unbind(dim=2)\n",
        "        q = q.transpose(1, 2)\n",
        "        k = k.transpose(1, 2)\n",
        "        v = v.transpose(1, 2)\n",
        "        scale = 1.0 / math.sqrt(self.d_head)\n",
        "        # PyTorch SDPA (uses flash when available)\n",
        "        y = F.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout.p if self.training else 0.0, is_causal=True)\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
        "        y = self.proj(y)\n",
        "        return y\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, n_embd: int, mult: int = 4, dropout: float = 0.0):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, mult * n_embd),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(mult * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, n_embd: int, n_head: int, dropout: float):\n",
        "        super().__init__()\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.attn = CausalSelfAttention(n_embd, n_head, dropout)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "        self.ffn = FeedForward(n_embd, mult=4, dropout=dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln1(x))\n",
        "        x = x + self.ffn(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "# ---- Tiny GPT ----\n",
        "class GPT(nn.Module):\n",
        "    def __init__(self, vocab_size: int, block_size: int, n_layer: int = 4, n_head: int = 4, n_embd: int = 256, dropout: float = 0.0):\n",
        "        super().__init__()\n",
        "        self.block_size = block_size\n",
        "        self.tok_emb = nn.Embedding(vocab_size, n_embd)\n",
        "        self.pos_emb = nn.Embedding(block_size, n_embd)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.blocks = nn.ModuleList([Block(n_embd, n_head, dropout) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd)\n",
        "        self.head = nn.Linear(n_embd, vocab_size, bias=False)\n",
        "\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            nn.init.normal_(m.weight, mean=0.0, std=0.02)\n",
        "            if m.bias is not None:\n",
        "                nn.init.zeros_(m.bias)\n",
        "        elif isinstance(m, nn.Embedding):\n",
        "            nn.init.normal_(m.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx: torch.Tensor, targets: torch.Tensor | None = None):\n",
        "        B, T = idx.shape\n",
        "        assert T <= self.block_size\n",
        "        #T = T[-self.block_size:]\n",
        "        pos = torch.arange(0, T, device=idx.device).unsqueeze(0)\n",
        "        x = self.tok_emb(idx) + self.pos_emb(pos)\n",
        "        x = self.drop(x)\n",
        "        for blk in self.blocks:\n",
        "            x = blk(x)\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.head(x)\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
        "        return logits, loss\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, idx: torch.Tensor, max_new_tokens: int = 200, temperature: float = 1.0,\n",
        "                top_k: int | None = 50, top_p: float | None = None):\n",
        "\n",
        "        self.eval()\n",
        "        # Guard: if the prompt is empty, start with a newline byte (10)\n",
        "        if idx.size(1) == 0:\n",
        "            idx = torch.full((idx.size(0), 1), 10, dtype=torch.long, device=idx.device)\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_cond = idx[:, -self.block_size:]\n",
        "            logits, _ = self(idx_cond)\n",
        "            logits = logits[:, -1, :] / max(temperature, 1e-6)\n",
        "            logits = top_k_top_p_filtering(logits, top_k=top_k, top_p=top_p)\n",
        "            probs = torch.softmax(logits, dim=-1)\n",
        "            next_id = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat([idx, next_id], dim=1)\n",
        "        return idx"
      ],
      "metadata": {
        "id": "iQXbAVgRq36H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import annotations\n",
        "import argparse, time, os, sys\n",
        "import torch\n",
        "# from tokenizer import ByteTokenizer\n",
        "# from dataset import ByteDataset\n",
        "# from model_gpt import GPT\n",
        "\n",
        "def estimate_loss(model: GPT, ds: ByteDataset, args) -> dict:\n",
        "    model.eval()\n",
        "    out = {}\n",
        "    with torch.no_grad():\n",
        "        for split in ['train', 'val']:\n",
        "            losses = []\n",
        "            for _ in range(args.eval_iters):\n",
        "                xb, yb = ds.get_batch(split, args.batch_size, args.device)\n",
        "                _, loss = model(xb, yb)\n",
        "                losses.append(loss.item())\n",
        "            out[split] = sum(losses) / len(losses)\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "def main(argv=None):\n",
        "    if argv is None:\n",
        "        argv = sys.argv[1:]\n",
        "\n",
        "    p = argparse.ArgumentParser()\n",
        "    p.add_argument('--data', type=str, required=True)\n",
        "    p.add_argument('--out_dir', type=str, default='runs/min-gpt')\n",
        "    p.add_argument('--block_size', type=int, default=256)\n",
        "    p.add_argument('--batch_size', type=int, default=32)\n",
        "    p.add_argument('--n_layer', type=int, default=4)\n",
        "    p.add_argument('--n_head', type=int, default=4)\n",
        "    p.add_argument('--n_embd', type=int, default=256)\n",
        "    p.add_argument('--dropout', type=float, default=0.0)\n",
        "    p.add_argument('--steps', type=int, default=2000)\n",
        "    p.add_argument('--lr', type=float, default=3e-4)\n",
        "    p.add_argument('--weight_decay', type=float, default=0.1)\n",
        "    p.add_argument('--grad_clip', type=float, default=1.0)\n",
        "    p.add_argument('--eval_interval', type=int, default=200)\n",
        "    p.add_argument('--eval_iters', type=int, default=50)\n",
        "    p.add_argument('--sample_every', type=int, default=200)\n",
        "    p.add_argument('--sample_tokens', type=int, default=256)\n",
        "    p.add_argument('--temperature', type=float, default=1.0)\n",
        "    p.add_argument('--top_k', type=int, default=50)\n",
        "    p.add_argument('--top_p', type=float, default=None)\n",
        "    p.add_argument('--cpu', action='store_true')\n",
        "    p.add_argument('--compile', action='store_true')\n",
        "    p.add_argument('--amp', action='store_true')\n",
        "    args = p.parse_args(argv)\n",
        "\n",
        "    args.device = torch.device('cuda' if torch.cuda.is_available() and not args.cpu else 'cpu')\n",
        "\n",
        "    tok = ByteTokenizer()\n",
        "    ds = ByteDataset(args.data, block_size=args.block_size)\n",
        "    model = GPT(tok.vocab_size, args.block_size, args.n_layer, args.n_head, args.n_embd, args.dropout).to(args.device)\n",
        "\n",
        "    if args.compile and hasattr(torch, 'compile'):\n",
        "        model = torch.compile(model)\n",
        "\n",
        "    opt = torch.optim.AdamW(model.parameters(), lr=args.lr, betas=(0.9, 0.95), weight_decay=args.weight_decay)\n",
        "    scaler = torch.cuda.amp.GradScaler(enabled=(args.amp and args.device.type == 'cuda'))\n",
        "\n",
        "    best_val = float('inf')\n",
        "    t0 = time.time()\n",
        "    model.train()\n",
        "    for step in range(1, args.steps + 1):\n",
        "        xb, yb = ds.get_batch('train', args.batch_size, args.device)\n",
        "        with torch.cuda.amp.autocast(enabled=(args.amp and args.device.type == 'cuda')):\n",
        "            _, loss = model(xb, yb)\n",
        "        opt.zero_grad(set_to_none=True)\n",
        "        scaler.scale(loss).backward()\n",
        "        if args.grad_clip > 0:\n",
        "            scaler.unscale_(opt)\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), args.grad_clip)\n",
        "        scaler.step(opt)\n",
        "        scaler.update()\n",
        "\n",
        "        if step % 50 == 0:\n",
        "            print(f\"step {step:5d} | loss {loss.item():.4f} | {(time.time()-t0):.1f}s\")\n",
        "            t0 = time.time()\n",
        "\n",
        "        if step % args.eval_interval == 0:\n",
        "            losses = estimate_loss(model, ds, args)\n",
        "            print(f\"eval | train {losses['train']:.4f} | val {losses['val']:.4f}\")\n",
        "            if losses['val'] < best_val:\n",
        "                best_val = losses['val']\n",
        "                ckpt_path = f\"{args.out_dir}/model_best.pt\"\n",
        "                os.makedirs(args.out_dir, exist_ok=True)\n",
        "                torch.save({'model': model.state_dict(), 'config': {\n",
        "                    'vocab_size': tok.vocab_size,\n",
        "                    'block_size': args.block_size,\n",
        "                    'n_layer': args.n_layer,\n",
        "                    'n_head': args.n_head,\n",
        "                    'n_embd': args.n_embd,\n",
        "                    'dropout': args.dropout,\n",
        "                }}, ckpt_path)\n",
        "                print(f\"saved checkpoint: {ckpt_path}\")\n",
        "\n",
        "        if args.sample_every > 0 and step % args.sample_every == 0:\n",
        "            start = torch.randint(low=0, high=len(ds.train) - args.block_size - 1, size=(1,)).item()\n",
        "            seed = ds.train[start:start + args.block_size].unsqueeze(0).to(args.device)\n",
        "            out = model.generate(seed, max_new_tokens=args.sample_tokens, temperature=args.temperature, top_k=args.top_k, top_p=args.top_p)\n",
        "            txt = tok.decode(out[0].cpu())\n",
        "            print(\"\\n================ SAMPLE ================\\n\" + txt[-(args.block_size + args.sample_tokens):] + \"\\n=======================================\\n\")\n",
        "\n",
        "    # final save\n",
        "    os.makedirs(args.out_dir, exist_ok=True)\n",
        "    torch.save({'model': model.state_dict()}, f\"{args.out_dir}/model_final.pt\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main([\n",
        "        '--data', 'tiny_hi.txt',\n",
        "        '--steps', '400',\n",
        "        '--sample_every', '100',\n",
        "        '--eval_interval', '100',\n",
        "        '--batch_size', '32',\n",
        "        '--block_size', '128',\n",
        "        '--n_layer', '2',\n",
        "        '--n_head', '2',\n",
        "        '--n_embd', '128'\n",
        "    ])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HZwjuzwZr7Y5",
        "outputId": "ac63f569-0e45-4b70-99f0-4fc1cc07581a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2884841092.py:61: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(args.amp and args.device.type == 'cuda'))\n",
            "/tmp/ipython-input-2884841092.py:68: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=(args.amp and args.device.type == 'cuda')):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step    50 | loss 2.2778 | 13.8s\n",
            "step   100 | loss 1.4588 | 13.3s\n",
            "eval | train 1.4686 | val 1.4469\n",
            "saved checkpoint: runs/min-gpt/model_best.pt\n",
            "\n",
            "================ SAMPLE ================\n",
            "ा। कबहुँ न संत करहिं तेहि पाना॥\n",
            "सुरसरि मिलें सो पतअन स क। ादु। लनंी कडररबाअन सर ु अाेबल ु ोऍ र द मह सय़ रिबपरमितखलससह  आग॥पररमुता ढअ\n",
            " ं &\n",
            "=======================================\n",
            "\n",
            "step   150 | loss 1.3503 | 23.6s\n",
            "step   200 | loss 1.3252 | 13.4s\n",
            "eval | train 1.3000 | val 1.2839\n",
            "saved checkpoint: runs/min-gpt/model_best.pt\n",
            "\n",
            "================ SAMPLE ================\n",
            "लकु बधजोगू॥\n",
            "बाल बिलोकि बहुत मैं बाँचा। अब यहु मरलार न्ु सिभचछो-। बसरनलामिीबरं॥हईप। त ौि \n",
            "\n",
            "हनमदल ीिल हं गाोंि ्रबए कनृखल बाि ासाोससससईा।हतइ \n",
            "=======================================\n",
            "\n",
            "step   250 | loss 1.2376 | 23.5s\n",
            "step   300 | loss 1.2083 | 13.5s\n",
            "eval | train 1.2039 | val 1.1900\n",
            "saved checkpoint: runs/min-gpt/model_best.pt\n",
            "\n",
            "================ SAMPLE ================\n",
            "ृपाला। ईस अंस भव परम कृपाला॥\n",
            "सुनि सनमानहिं सबहि ्र नाउमंर बमिो भ सा ाकमुर।ुबखवी  छरुं आ॥ क हकँइिक बब महुंस्ं दभिंोनि ह कहर वर मस ोत ईपसभा बनसी ि\n",
            "=======================================\n",
            "\n",
            "step   350 | loss 1.2009 | 23.6s\n",
            "step   400 | loss 1.1337 | 13.5s\n",
            "eval | train 1.1271 | val 1.1161\n",
            "saved checkpoint: runs/min-gpt/model_best.pt\n",
            "\n",
            "================ SAMPLE ================\n",
            " उघारा। चितवत कामु भयउ जरि छारा॥३॥\n",
            "हाहाकार भयउ जरिि भलुरानि्नुो-ब्भिपहु सं किहं हाहो॥ुं राकउ ब्हाोिी बर मधँइ बृिलु कराुगिि रबि पखल हष्े अदहिाान \n",
            "=======================================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import annotations\n",
        "import argparse, torch, sys\n",
        "\n",
        "\n",
        "def main(argv=None):\n",
        "    if argv is None:\n",
        "        argv = sys.argv[1:]\n",
        "\n",
        "    p = argparse.ArgumentParser()\n",
        "    p.add_argument('--ckpt', type=str, required=True)\n",
        "    p.add_argument('--prompt', type=str, default='')\n",
        "    p.add_argument('--tokens', type=int, default=200)\n",
        "    p.add_argument('--temperature', type=float, default=1.0)\n",
        "    p.add_argument('--top_k', type=int, default=50)\n",
        "    p.add_argument('--top_p', type=float, default=None)\n",
        "    p.add_argument('--cpu', action='store_true')\n",
        "    args = p.parse_args(argv)\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() and not args.cpu else 'cpu')\n",
        "\n",
        "    tok = ByteTokenizer()\n",
        "    prompt_ids = tok.encode(args.prompt).unsqueeze(0).to(device)\n",
        "    if prompt_ids.numel() == 0:\n",
        "        # If no prompt provided, seed with newline byte (10)\n",
        "        prompt_ids = torch.tensor([[10]], dtype=torch.long, device=device)\n",
        "\n",
        "    ckpt = torch.load(args.ckpt, map_location=device)\n",
        "    config = ckpt.get('config', None)\n",
        "\n",
        "    if config is None:\n",
        "        # fallback defaults\n",
        "        model = GPT(tok.vocab_size, block_size=256).to(device)\n",
        "        model.load_state_dict(ckpt['model'])\n",
        "    else:\n",
        "        model = GPT(**config).to(device)\n",
        "        model.load_state_dict(ckpt['model'])\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        out = model.generate(prompt_ids, max_new_tokens=args.tokens, temperature=args.temperature, top_k=args.top_k, top_p=args.top_p)\n",
        "\n",
        "    print(tok.decode(out[0].cpu()))\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main([\n",
        "        '--ckpt', 'runs/min-gpt/model_best.pt',\n",
        "        '--tokens', '200',\n",
        "        '--prompt', 'करउ अनुग्रह'\n",
        "    ])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V-ANgUzSv5Qn",
        "outputId": "168e7ddd-d318-416c-c11a-c5e603e2ad3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "करउ अनुग्रहू गन भिहोहत\n",
            "ो म बत नबग ासाेनउ गिोहधषो ो। पनुट पर े समस ्सस सिप्ेरेव बुनिघ कीक \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import annotations\n",
        "import argparse, torch, sys\n",
        "\n",
        "\n",
        "def main(argv=None):\n",
        "    if(argv is None):\n",
        "        argv = sys.argv[1:]\n",
        "    p = argparse.ArgumentParser()\n",
        "    p.add_argument('--data', type=str, required=True)\n",
        "    p.add_argument('--ckpt', type=str, required=True)\n",
        "    p.add_argument('--block_size', type=int, default=256)\n",
        "    p.add_argument('--batch_size', type=int, default=32)\n",
        "    p.add_argument('--iters', type=int, default=100)\n",
        "    p.add_argument('--cpu', action='store_true')\n",
        "    args = p.parse_args(argv)\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() and not args.cpu else 'cpu')\n",
        "\n",
        "    ds = ByteDataset(args.data, block_size=args.block_size)\n",
        "    ckpt = torch.load(args.ckpt, map_location=device)\n",
        "    cfg = ckpt.get('config', {\n",
        "        'vocab_size': 256,\n",
        "        'block_size': args.block_size,\n",
        "        'n_layer': 4,\n",
        "        'n_head': 4,\n",
        "        'n_embd': 256,\n",
        "        'dropout': 0.0,\n",
        "    })\n",
        "    model = GPT(**cfg).to(device)\n",
        "    model.load_state_dict(ckpt['model'])\n",
        "\n",
        "    model.eval()\n",
        "    losses = []\n",
        "    with torch.no_grad():\n",
        "        for _ in range(args.iters):\n",
        "            xb, yb = ds.get_batch('val', args.batch_size, device)\n",
        "            _, loss = model(xb, yb)\n",
        "            losses.append(loss.item())\n",
        "    print(f\"val loss: {sum(losses)/len(losses):.4f}\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main([\n",
        "        '--data', 'tiny_hi.txt', '--ckpt', 'runs/min-gpt/model_best.pt', '--iters', '50' ,'--block_size',' 128'\n",
        "    ])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5_Za07Zsz9tb",
        "outputId": "19987d19-d3f0-45eb-ed71-ddaecfc75988"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val loss: 1.1151\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sip11LYz12xN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}