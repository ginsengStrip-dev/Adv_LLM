{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3UJZnMdbJbOM"
      },
      "source": [
        "**Tokenizer BPE normal**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "Zpj4VPgsFX5D"
      },
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "import os, json\n",
        "from pathlib import Path\n",
        "from typing import List, Union\n",
        "\n",
        "try:\n",
        "    from tokenizers import ByteLevelBPETokenizer, Tokenizer\n",
        "except Exception:\n",
        "    ByteLevelBPETokenizer = None\n",
        "\n",
        "class BPETokenizer:\n",
        "    \"\"\"Minimal BPE wrapper (HuggingFace tokenizers).\n",
        "    Trains on a text file or a folder of .txt files. Saves merges/vocab to out_dir.\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size: int = 32000, special_tokens: List[str] | None = None):\n",
        "        if ByteLevelBPETokenizer is None:\n",
        "            raise ImportError(\"Please `pip install tokenizers` for BPETokenizer.\")\n",
        "        self.vocab_size = vocab_size\n",
        "        self.special_tokens = special_tokens or [\"<s>\", \"</s>\", \"<pad>\", \"<unk>\", \"<mask>\"]\n",
        "        self._tok = None\n",
        "\n",
        "    def train(self, data_path: Union[str, Path]):\n",
        "        files: List[str] = []\n",
        "        p = Path(data_path)\n",
        "        if p.is_dir():\n",
        "            files = [str(fp) for fp in p.glob(\"**/*.txt\")]\n",
        "        else:\n",
        "            files = [str(p)]\n",
        "        tok = ByteLevelBPETokenizer()\n",
        "        tok.train(files=files, vocab_size=self.vocab_size, min_frequency=2, special_tokens=self.special_tokens)\n",
        "        self._tok = tok\n",
        "\n",
        "    def save(self, out_dir: Union[str, Path]):\n",
        "        out = Path(out_dir); out.mkdir(parents=True, exist_ok=True)\n",
        "        assert self._tok is not None, \"Train or load before save().\"\n",
        "        self._tok.save_model(str(out))\n",
        "        self._tok.save(str(out / \"tokenizer.json\"))\n",
        "        meta = {\"vocab_size\": self.vocab_size, \"special_tokens\": self.special_tokens}\n",
        "        (out/\"bpe_meta.json\").write_text(json.dumps(meta))\n",
        "\n",
        "    def load(self, dir_path: Union[str, Path]):\n",
        "        dirp = Path(dir_path)\n",
        "        # Prefer explicit filenames; fall back to glob if needed.\n",
        "        vocab = dirp / \"vocab.json\"\n",
        "        merges = dirp / \"merges.txt\"\n",
        "        tokenizer = dirp / \"tokenizer.json\"\n",
        "        if not vocab.exists() or not merges.exists():\n",
        "            # Fallback for custom basenames\n",
        "            vs = list(dirp.glob(\"*.json\"))\n",
        "            ms = list(dirp.glob(\"*.txt\"))\n",
        "            if not vs or not ms:\n",
        "                raise FileNotFoundError(f\"Could not find vocab.json/merges.txt in {dirp}\")\n",
        "            vocab = vs[0]\n",
        "            merges = ms[0]\n",
        "        # tok = ByteLevelBPETokenizer(str(vocab), str(merges))\n",
        "        tok = Tokenizer.from_file(str(tokenizer))\n",
        "        self._tok = tok\n",
        "        meta_file = dirp / \"bpe_meta.json\"\n",
        "        if meta_file.exists():\n",
        "            meta = json.loads(meta_file.read_text())\n",
        "            self.vocab_size = meta.get(\"vocab_size\", self.vocab_size)\n",
        "            self.special_tokens = meta.get(\"special_tokens\", self.special_tokens)\n",
        "\n",
        "\n",
        "    def encode(self, text: str):\n",
        "        ids = self._tok.encode(text).ids\n",
        "        return ids\n",
        "\n",
        "    def decode(self, ids):\n",
        "        return self._tok.decode(ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BE8mEurXJi3q"
      },
      "source": [
        "**LR_scheduler**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "E1ZdX89WHPov"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "class WarmupCosineLR:\n",
        "    \"\"\"Linear warmup → cosine decay (per-step API).\"\"\"\n",
        "    def __init__(self, optimizer, warmup_steps: int, total_steps: int, base_lr: float):\n",
        "        self.optimizer = optimizer\n",
        "        self.warmup_steps = max(1, warmup_steps)\n",
        "        self.total_steps = max(self.warmup_steps+1, total_steps)\n",
        "        self.base_lr = base_lr\n",
        "        self.step_num = 0\n",
        "    def step(self):\n",
        "        self.step_num += 1\n",
        "        if self.step_num <= self.warmup_steps:\n",
        "            lr = self.base_lr * self.step_num / self.warmup_steps\n",
        "        else:\n",
        "            progress = (self.step_num - self.warmup_steps) / (self.total_steps - self.warmup_steps)\n",
        "            lr = 0.5 * self.base_lr * (1.0 + math.cos(math.pi * progress))\n",
        "        for g in self.optimizer.param_groups:\n",
        "            g['lr'] = lr\n",
        "        return lr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lmq8tHg5Jl3y"
      },
      "source": [
        "**dataset_bpe**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "zy7VB7RUIsME"
      },
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from pathlib import Path\n",
        "from typing import Tuple\n",
        "# from tokenizer_bpe import BPETokenizer\n",
        "\n",
        "class TextBPEBuffer(Dataset):\n",
        "    \"\"\"Memory-mapped-ish single-file dataset: tokenize once → long tensor of ids.\n",
        "    get(idx) returns a (block_size,) slice; we construct (x,y) with shift inside collate.\n",
        "    \"\"\"\n",
        "    def __init__(self, path: str, tokenizer: BPETokenizer, block_size: int = 256):\n",
        "        super().__init__()\n",
        "        self.block_size = block_size\n",
        "        text = Path(path).read_text(encoding='utf-8')\n",
        "        self.ids = torch.tensor(tokenizer.encode(text), dtype=torch.long)\n",
        "    def __len__(self):\n",
        "        return max(0, self.ids.numel() - self.block_size - 1)\n",
        "    def __getitem__(self, i: int):\n",
        "        x = self.ids[i:i+self.block_size]\n",
        "        y = self.ids[i+1:i+self.block_size+1]\n",
        "        return x, y\n",
        "\n",
        "def make_loader(path: str, tokenizer: BPETokenizer, block_size: int, batch_size: int, shuffle=True) -> DataLoader:\n",
        "    ds = TextBPEBuffer(path, tokenizer, block_size)\n",
        "    return DataLoader(ds, batch_size=batch_size, shuffle=shuffle, drop_last=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OpAWTaauJuYK"
      },
      "source": [
        "**amp_accum**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "PdpKfKLUIyfz"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "class AmpGrad:\n",
        "    \"\"\"AMP + gradient accumulation wrapper.\n",
        "    Usage:\n",
        "        amp = AmpGrad(optimizer, accum=4, amp=True)\n",
        "        amp.backward(loss)\n",
        "        if amp.should_step(): amp.step(); amp.zero_grad()\n",
        "    \"\"\"\n",
        "    def __init__(self, optimizer, accum: int = 1, amp: bool = True):\n",
        "        self.optim = optimizer\n",
        "        self.accum = max(1, accum)\n",
        "        self.amp = amp and torch.cuda.is_available()\n",
        "        self.scaler = torch.cuda.amp.GradScaler(enabled=self.amp)\n",
        "        self._n = 0\n",
        "    def backward(self, loss: torch.Tensor):\n",
        "        loss = loss / self.accum\n",
        "        if self.amp:\n",
        "            self.scaler.scale(loss).backward()\n",
        "        else:\n",
        "            loss.backward()\n",
        "        self._n += 1\n",
        "    def should_step(self):\n",
        "        return (self._n % self.accum) == 0\n",
        "    def step(self):\n",
        "        if self.amp:\n",
        "            self.scaler.step(self.optim)\n",
        "            self.scaler.update()\n",
        "        else:\n",
        "            self.optim.step()\n",
        "    def zero_grad(self):\n",
        "        self.optim.zero_grad(set_to_none=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k33SHJDMJzXS"
      },
      "source": [
        "**checkpointing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "BzAOjdO2I4Mz"
      },
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "import os\n",
        "from pathlib import Path\n",
        "from typing import Any, Dict, Optional, Tuple\n",
        "import sys\n",
        "# sys.path.append(str(Path(__file__).resolve().parents[1]/'part_3'))\n",
        "import time\n",
        "import torch\n",
        "import shutil\n",
        "import torch.nn as nn\n",
        "\n",
        "DEF_NAME = \"model_last.pt\"\n",
        "\n",
        "# ----------------------------- TB-only helpers (safe no-ops otherwise) ----------------------------- #\n",
        "def _is_tb(logger) -> bool:\n",
        "    return getattr(logger, \"w\", None) is not None\n",
        "\n",
        "\n",
        "# checkpointing._log_hparams_tb\n",
        "def _log_hparams_tb(logger, args, total_steps):\n",
        "    if not _is_tb(logger): return\n",
        "    try:\n",
        "        h = dict(\n",
        "            vocab_size=args.vocab_size, block_size=args.block_size, n_layer=args.n_layer,\n",
        "            n_head=args.n_head, n_embd=args.n_embd, dropout=args.dropout, lr=args.lr,\n",
        "            warmup_steps=args.warmup_steps, batch_size=args.batch_size, grad_accum=args.grad_accum_steps,\n",
        "            mixed_precision=args.mixed_precision, steps=args.steps, epochs=args.epochs,\n",
        "        )\n",
        "        logger.hparams(h, {\"meta/total_steps\": float(total_steps)})\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "def _maybe_log_graph_tb(logger, model, xb, yb):\n",
        "    if not hasattr(logger, \"graph\"):\n",
        "        return\n",
        "    try:\n",
        "        class _TensorOnly(nn.Module):\n",
        "            def __init__(self, m):\n",
        "                super().__init__(); self.m = m.eval()\n",
        "            def forward(self, x, y=None):\n",
        "                out = self.m(x, y) if y is not None else self.m(x)\n",
        "                if isinstance(out, (list, tuple)):\n",
        "                    for o in out:\n",
        "                        if torch.is_tensor(o):\n",
        "                            return o\n",
        "                    return out[0]\n",
        "                return out\n",
        "        wrapped = _TensorOnly(model).to(xb.device)\n",
        "        logger.graph(wrapped, (xb, yb))\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "def _log_model_stats(logger, model, step: int, do_hists: bool = False):\n",
        "    if not _is_tb(logger): return\n",
        "    try:\n",
        "        params = [p for p in model.parameters() if p.requires_grad]\n",
        "        total_param_norm = torch.norm(torch.stack([p.detach().norm(2) for p in params]), 2).item()\n",
        "        grads = [p.grad for p in params if p.grad is not None]\n",
        "        total_grad_norm = float('nan')\n",
        "        if grads:\n",
        "            total_grad_norm = torch.norm(torch.stack([g.detach().norm(2) for g in grads]), 2).item()\n",
        "        logger.log(step=step, **{\n",
        "            \"train/param_global_l2\": total_param_norm,\n",
        "            \"train/grad_global_l2\": total_grad_norm,\n",
        "        })\n",
        "        if do_hists:\n",
        "            for name, p in model.named_parameters():\n",
        "                logger.hist(f\"params/{name}\", p, step)\n",
        "                if p.grad is not None:\n",
        "                    logger.hist(f\"grads/{name}\", p.grad, step)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "def _maybe_log_attention(logger, model, xb, step: int, every: int = 100):\n",
        "    \"\"\"\n",
        "    Logs Q/K/V histograms for each Transformer block using the current minibatch xb.\n",
        "    No model edits. No hooks. Runs a light no-grad recomputation of the pre-attn path.\n",
        "    - Takes first batch and first head only to keep logs tiny.\n",
        "    - Uses pre-RoPE values (simpler & stable for histograms).\n",
        "    \"\"\"\n",
        "    if not _is_tb(logger) or step == 0 or (step % every):\n",
        "        return\n",
        "    try:\n",
        "        import torch\n",
        "        with torch.no_grad(), torch.cuda.amp.autocast(enabled=False):\n",
        "            # Recreate inputs seen by blocks\n",
        "            x = model.tok_emb(xb)           # (B,T,C)\n",
        "            x = model.drop(x)\n",
        "\n",
        "            B, T, _ = x.shape\n",
        "            for li, blk in enumerate(getattr(model, \"blocks\", [])):\n",
        "                h = blk.ln1(x)              # pre-attn normalized hidden\n",
        "\n",
        "                attn = blk.attn\n",
        "                # Project to Q/K/V exactly like the module (pre-RoPE for simplicity)\n",
        "                q = attn.wq(h).view(B, T, attn.n_head,   attn.d_head).transpose(1, 2)      # (B,H,T,D)\n",
        "                k = attn.wk(h).view(B, T, attn.n_kv_head, attn.d_head).transpose(1, 2)     # (B,Hk,T,D)\n",
        "                v = attn.wv(h).view(B, T, attn.n_kv_head, attn.d_head).transpose(1, 2)     # (B,Hk,T,D)\n",
        "\n",
        "                # Take a tiny slice to keep logs light\n",
        "                q1 = q[:1, :1].contiguous().view(-1).float().cpu()\n",
        "                k1 = k[:1, :1].contiguous().view(-1).float().cpu()\n",
        "                v1 = v[:1, :1].contiguous().view(-1).float().cpu()\n",
        "\n",
        "                # Drop non-finite (defensive)\n",
        "                q1 = q1[torch.isfinite(q1)]\n",
        "                k1 = k1[torch.isfinite(k1)]\n",
        "                v1 = v1[torch.isfinite(v1)]\n",
        "\n",
        "                if q1.numel() > 0: logger.hist(f\"qkv/block{li}/q_hist\", q1, step)\n",
        "                if k1.numel() > 0: logger.hist(f\"qkv/block{li}/k_hist\", k1, step)\n",
        "                if v1.numel() > 0: logger.hist(f\"qkv/block{li}/v_hist\", v1, step)\n",
        "\n",
        "                # Optional small scalars (norms) that show up on Time Series\n",
        "                if q1.numel(): logger.log(step=step, **{f\"qkv/block{li}/q_l2_mean\": float(q1.square().mean().sqrt())})\n",
        "                if k1.numel(): logger.log(step=step, **{f\"qkv/block{li}/k_l2_mean\": float(k1.square().mean().sqrt())})\n",
        "                if v1.numel(): logger.log(step=step, **{f\"qkv/block{li}/v_l2_mean\": float(v1.square().mean().sqrt())})\n",
        "\n",
        "                # Advance x to next block with a CHEAP approximation to avoid doubling full compute:\n",
        "                # use the model's own FFN path only; skip re-running attention (we're only logging pre-attn stats).\n",
        "                x = x + blk.ffn(blk.ln2(x))\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"[qkv] logging failed: {e}\")\n",
        "\n",
        "\n",
        "def _log_runtime(logger, step: int, it_t0: float, xb, device):\n",
        "    try:\n",
        "        dt = time.time() - it_t0\n",
        "        toks = int(xb.numel())\n",
        "        toks_per_s = toks / max(dt, 1e-6)\n",
        "        mem = torch.cuda.memory_allocated()/(1024**2) if torch.cuda.is_available() else 0.0\n",
        "        logger.log(step=step, **{\n",
        "            \"sys/throughput_tokens_per_s\": toks_per_s,\n",
        "            \"sys/step_time_s\": dt,\n",
        "            \"sys/gpu_mem_alloc_mb\": mem\n",
        "        })\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "def _log_samples_tb(logger, model, tok, xb, device, step: int, max_new_tokens: int = 64):\n",
        "    if not _is_tb(logger): return\n",
        "    if tok is None: return\n",
        "    try:\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            out = model.generate(xb[:1].to(device), max_new_tokens=max_new_tokens, temperature=1.0, top_k=50)\n",
        "        model.train()\n",
        "        text = tok.decode(out[0].tolist())\n",
        "        logger.text(\"samples/generation\", text, step)\n",
        "    except Exception:\n",
        "        pass\n",
        "# ---------------------------------------------------------------------- #\n",
        "\n",
        "def _extract_config_from_model(model) -> dict:\n",
        "    \"\"\"\n",
        "    Best-effort extraction of GPTModern-like config including GQA fields.\n",
        "    \"\"\"\n",
        "    cfg = {}\n",
        "    try:\n",
        "        tok_emb = getattr(model, \"tok_emb\", None)\n",
        "        blocks = getattr(model, \"blocks\", None)\n",
        "        if tok_emb is None or not blocks:\n",
        "            return cfg\n",
        "\n",
        "        try:\n",
        "            from swiglu import SwiGLU  # optional\n",
        "        except Exception:\n",
        "            class SwiGLU: pass\n",
        "\n",
        "        cfg[\"vocab_size\"] = int(tok_emb.num_embeddings)\n",
        "        cfg[\"block_size\"]  = int(getattr(model, \"block_size\", 0) or 0)\n",
        "        cfg[\"n_layer\"]     = int(len(blocks))\n",
        "\n",
        "        first_blk = blocks[0]\n",
        "        attn = getattr(first_blk, \"attn\", None)\n",
        "        if attn is None:\n",
        "            return cfg\n",
        "\n",
        "        # Heads & dims\n",
        "        cfg[\"n_head\"]   = int(getattr(attn, \"n_head\"))\n",
        "        d_head          = int(getattr(attn, \"d_head\"))\n",
        "        cfg[\"n_embd\"]   = int(cfg[\"n_head\"] * d_head)\n",
        "        cfg[\"n_kv_head\"]= int(getattr(attn, \"n_kv_head\", cfg[\"n_head\"]))  # default to MHA\n",
        "\n",
        "        # Dropout (if present)\n",
        "        drop = getattr(attn, \"dropout\", None)\n",
        "        cfg[\"dropout\"] = float(getattr(drop, \"p\", 0.0)) if drop is not None else 0.0\n",
        "\n",
        "        # Norm/FFN style\n",
        "        cfg[\"use_rmsnorm\"] = isinstance(getattr(model, \"ln_f\", None), nn.Identity)\n",
        "        cfg[\"use_swiglu\"]  = isinstance(getattr(first_blk, \"ffn\", None), SwiGLU)\n",
        "\n",
        "        # Positional / attention tricks\n",
        "        for k in (\"rope\", \"max_pos\", \"sliding_window\", \"attention_sink\"):\n",
        "            if hasattr(attn, k):\n",
        "                val = getattr(attn, k)\n",
        "                cfg[k] = int(val) if isinstance(val, bool) else val\n",
        "    except Exception:\n",
        "        return {}\n",
        "    return cfg\n",
        "\n",
        "def _verify_model_matches(model, cfg: Dict[str, Any]) -> Tuple[bool, str]:\n",
        "    \"\"\"Return (ok, message).\"\"\"\n",
        "    expected = {\n",
        "        \"block_size\": cfg.get(\"block_size\"),\n",
        "        \"n_layer\":    cfg.get(\"n_layer\"),\n",
        "        \"n_head\":     cfg.get(\"n_head\"),\n",
        "        \"n_embd\":     cfg.get(\"n_embd\"),\n",
        "        \"vocab_size\": cfg.get(\"vocab_size\"),\n",
        "        \"n_kv_head\":  cfg.get(\"n_kv_head\", cfg.get(\"n_head\")),\n",
        "    }\n",
        "    got = {\n",
        "        \"block_size\": int(getattr(model, \"block_size\", -1)),\n",
        "        \"n_layer\":    int(len(model.blocks)),\n",
        "        \"vocab_size\": int(model.tok_emb.num_embeddings),\n",
        "    }\n",
        "    first_blk = model.blocks[0]\n",
        "    got.update({\n",
        "        \"n_head\":     int(first_blk.attn.n_head),\n",
        "        \"n_embd\":     int(first_blk.attn.n_head * first_blk.attn.d_head),\n",
        "        \"n_kv_head\":  int(getattr(first_blk.attn, \"n_kv_head\", first_blk.attn.n_head)),\n",
        "    })\n",
        "    diffs = [f\"{k}: ckpt={expected[k]} vs model={got[k]}\" for k in expected if expected[k] != got[k]]\n",
        "    if diffs:\n",
        "        return False, \"Architecture mismatch:\\n  \" + \"\\n  \".join(diffs)\n",
        "    return True, \"ok\"\n",
        "\n",
        "\n",
        "def save_checkpoint(model, optimizer, scheduler, amp, step: int, out_dir: str,\n",
        "                    tokenizer_dir: str | None = None, config: dict | None = None):\n",
        "    out = Path(out_dir); out.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Prefer the model’s own config if available (e.g., a dict or dataclass with __dict__/asdict)\n",
        "    if hasattr(model, \"config\"):\n",
        "        cfg_obj = model.config\n",
        "        cfg = dict(cfg_obj) if isinstance(cfg_obj, dict) else getattr(cfg_obj, \"__dict__\", None) or _extract_config_from_model(model)\n",
        "    else:\n",
        "        cfg = config if config is not None else _extract_config_from_model(model)\n",
        "\n",
        "    torch.save({\n",
        "        \"model\": model.state_dict(),\n",
        "        \"optimizer\": optimizer.state_dict() if optimizer is not None else None,\n",
        "        \"scheduler\": scheduler.state_dict() if hasattr(scheduler, \"state_dict\") else None,\n",
        "        \"amp_scaler\": amp.scaler.state_dict() if amp and getattr(amp, \"scaler\", None) else None,\n",
        "        \"step\": int(step),\n",
        "        \"config\": cfg,   # ← always write config\n",
        "        \"version\": \"part4-v2\",\n",
        "    }, out / DEF_NAME)\n",
        "\n",
        "    if tokenizer_dir is not None:\n",
        "        (out / \"tokenizer_dir.txt\").write_text(tokenizer_dir)\n",
        "\n",
        "\n",
        "\n",
        "def load_checkpoint(model, path: str, optimizer=None, scheduler=None, amp=None, strict: bool = True):\n",
        "    ckpt = torch.load(path, map_location=\"cpu\")\n",
        "\n",
        "    cfg = ckpt.get(\"config\")\n",
        "    if cfg:\n",
        "        ok, msg = _verify_model_matches(model, cfg)\n",
        "        if not ok:\n",
        "            raise RuntimeError(msg + \"\\nRebuild the model with this config, or load with strict=False.\")\n",
        "    else:\n",
        "        # Legacy checkpoint without config: strongly encourage a rebuild step elsewhere\n",
        "        print(\"[compat] Warning: checkpoint has no config; cannot verify architecture.\")\n",
        "\n",
        "    missing, unexpected = model.load_state_dict(ckpt[\"model\"], strict=strict)\n",
        "    if strict and (missing or unexpected):\n",
        "        raise RuntimeError(f\"State dict mismatch:\\n  missing: {missing}\\n  unexpected: {unexpected}\")\n",
        "\n",
        "    if optimizer is not None and ckpt.get(\"optimizer\") is not None:\n",
        "        optimizer.load_state_dict(ckpt[\"optimizer\"])\n",
        "    if scheduler is not None and ckpt.get(\"scheduler\") is not None and hasattr(scheduler, \"load_state_dict\"):\n",
        "        scheduler.load_state_dict(ckpt[\"scheduler\"])\n",
        "    if amp is not None and ckpt.get(\"amp_scaler\") is not None and getattr(amp, \"scaler\", None):\n",
        "        amp.scaler.load_state_dict(ckpt[\"amp_scaler\"])\n",
        "\n",
        "    return ckpt.get(\"step\", 0)\n",
        "\n",
        "\n",
        "# ----------------------------- checkpoint/save utils ----------------------------- #\n",
        "def checkpoint_paths(out_dir: Path, step: int):\n",
        "    return out_dir / f\"model_step{step:07d}.pt\", out_dir / \"model_last.pt\"\n",
        "\n",
        "def atomic_save_all(model, optim, sched, amp, step: int, out_dir: Path,\n",
        "                    tok_dir: str | None, keep_last_k: int, config: dict):\n",
        "    \"\"\"Write model_last.pt (with config) + a rolling per-step copy.\"\"\"\n",
        "    save_checkpoint(model, optim, sched, amp, step, str(out_dir), tok_dir, config=config)  # writes model_last.pt\n",
        "    per_step, last = checkpoint_paths(out_dir, step)\n",
        "    try:\n",
        "        shutil.copy2(last, per_step)\n",
        "    except Exception:\n",
        "        pass\n",
        "    # GC old per-step checkpoints\n",
        "    try:\n",
        "        ckpts = sorted(out_dir.glob(\"model_step*.pt\"))\n",
        "        for old in ckpts[:-keep_last_k]:\n",
        "            old.unlink(missing_ok=True)\n",
        "    except Exception:\n",
        "        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cswvXe7TKA36"
      },
      "source": [
        "**logger**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "H3rd1pqZJAxL"
      },
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "import time\n",
        "from pathlib import Path\n",
        "\n",
        "class NoopLogger:\n",
        "    def log(self, **kwargs):\n",
        "        pass\n",
        "    def close(self):\n",
        "        pass\n",
        "\n",
        "class TBLogger(NoopLogger):\n",
        "    \"\"\"\n",
        "    Backward compatible:\n",
        "      - logger.log(step=..., loss=..., lr=...)\n",
        "    Extras you can optionally use:\n",
        "      - logger.hist(\"params/wte.weight\", tensor, step)\n",
        "      - logger.text(\"samples/generation\", text, step)\n",
        "      - logger.image(\"attn/heatmap\", HWC_or_CHW_tensor_or_np, step)\n",
        "      - logger.graph(model, example_batch)\n",
        "      - logger.hparams(dict_of_config, dict_of_metrics_once)\n",
        "      - logger.flush()\n",
        "    Auto-behavior:\n",
        "      - If a value in .log(...) is a tensor/ndarray with >1 element, it logs a histogram.\n",
        "      - If key starts with \"text/\", logs as text.\n",
        "    \"\"\"\n",
        "    # logger.py\n",
        "    def __init__(self, out_dir: str, flush_secs: int = 10, run_name: str | None = None):\n",
        "        self.w = None\n",
        "        self.hparams_logged = False\n",
        "        run_name = run_name or time.strftime(\"%Y%m%d-%H%M%S\")\n",
        "        run_dir = Path(out_dir) / run_name\n",
        "        run_dir.mkdir(parents=True, exist_ok=True)\n",
        "        try:\n",
        "            from torch.utils.tensorboard import SummaryWriter\n",
        "            self.w = SummaryWriter(log_dir=str(run_dir), flush_secs=flush_secs)\n",
        "        except Exception as e:\n",
        "            print(f\"[TBLogger] TensorBoard not available: {e}. Logging disabled.\")\n",
        "        self._auto_hist_max_elems = 2048\n",
        "        self.run_dir = str(run_dir)  # handy for prints/debug\n",
        "\n",
        "\n",
        "\n",
        "    # ---------- backwards-compatible ----------\n",
        "    def log(self, step: Optional[int] = None, **kv: Any):\n",
        "        if not self.w: return\n",
        "        for k, v in kv.items():\n",
        "            # text channel (opt-in via key prefix)\n",
        "            if isinstance(k, str) and k.startswith(\"text/\"):\n",
        "                try:\n",
        "                    self.w.add_text(k[5:], str(v), global_step=step)\n",
        "                except Exception:\n",
        "                    pass\n",
        "                continue\n",
        "\n",
        "            # scalar vs histogram auto-route\n",
        "            try:\n",
        "                import torch, numpy as np  # lazy\n",
        "                is_torch = isinstance(v, torch.Tensor)\n",
        "                is_np = isinstance(v, np.ndarray)\n",
        "                if is_torch or is_np:\n",
        "                    # scalar?\n",
        "                    numel = int(v.numel() if is_torch else v.size)\n",
        "                    if numel == 1:\n",
        "                        val = (v.item() if is_torch else float(v))\n",
        "                        self.w.add_scalar(k, float(val), global_step=step)\n",
        "                    else:\n",
        "                        # small-ish tensors => histogram\n",
        "                        if numel <= self._auto_hist_max_elems:\n",
        "                            self.w.add_histogram(k, v.detach().cpu() if is_torch else v, global_step=step)\n",
        "                        else:\n",
        "                            # fall back to scalar summary stats\n",
        "                            arr = v.detach().cpu().flatten().numpy() if is_torch else v.flatten()\n",
        "                            self.w.add_scalar(k + \"/mean\", float(arr.mean()), global_step=step)\n",
        "                            self.w.add_scalar(k + \"/std\", float(arr.std()), global_step=step)\n",
        "                    continue\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "            # number-like\n",
        "            try:\n",
        "                self.w.add_scalar(k, float(v), global_step=step)\n",
        "            except Exception:\n",
        "                # swallow non-numeric junk silently (same behavior as before)\n",
        "                pass\n",
        "\n",
        "    # ---------- nice-to-have helpers ----------\n",
        "    def hist(self, tag: str, values: Any, step: Optional[int] = None, bins: str = \"tensorflow\"):\n",
        "        if not self.w: return\n",
        "        try:\n",
        "            import torch\n",
        "            if isinstance(values, torch.Tensor):\n",
        "                values = values.detach().cpu()\n",
        "            self.w.add_histogram(tag, values, global_step=step, bins=bins)\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    def text(self, tag: str, text: str, step: Optional[int] = None):\n",
        "        if not self.w: return\n",
        "        try:\n",
        "            self.w.add_text(tag, text, global_step=step)\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    def image(self, tag: str, img, step: Optional[int] = None):\n",
        "        \"\"\"\n",
        "        img: torch.Tensor [C,H,W] or [H,W,C] or numpy array\n",
        "        \"\"\"\n",
        "        if not self.w: return\n",
        "        try:\n",
        "            self.w.add_image(tag, img, global_step=step, dataformats=\"CHW\" if getattr(img, \"ndim\", 0) == 3 and img.shape[0] in (1,3) else \"HWC\")\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    def graph(self, model, example_input):\n",
        "        if not self.w: return\n",
        "        try:\n",
        "            # example_input: a Tensor batch or a tuple\n",
        "            if not isinstance(example_input, tuple):\n",
        "                example_input = (example_input,)\n",
        "            self.w.add_graph(model, example_input)\n",
        "        except Exception:\n",
        "            pass  # graph tracing can fail depending on model control flow; don't crash\n",
        "\n",
        "    def hparams(self, hparams: Dict[str, Any], metrics_once: Optional[Dict[str, float]] = None):\n",
        "        if not self.w or self.hparams_logged:\n",
        "            return\n",
        "        try:\n",
        "            # Single, stable sub-run so it doesn’t spam the left pane\n",
        "            self.w.add_hparams(hparams, metrics_once or {}, run_name=\"_hparams\")\n",
        "            self.hparams_logged = True\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    def flush(self):\n",
        "        if self.w:\n",
        "            try: self.w.flush()\n",
        "            except Exception: pass\n",
        "\n",
        "    def close(self):\n",
        "        if self.w:\n",
        "            try: self.w.close()\n",
        "            except Exception: pass\n",
        "\n",
        "class WBLogger(NoopLogger):\n",
        "    def __init__(self, project: str, run_name: str | None = None):\n",
        "        try:\n",
        "            import wandb\n",
        "            wandb.init(project=project, name=run_name)\n",
        "            self.wb = wandb\n",
        "        except Exception:\n",
        "            self.wb = None\n",
        "    def log(self, **kv):\n",
        "        if self.wb: self.wb.log(kv)\n",
        "\n",
        "\n",
        "def init_logger(which: str, out_dir: str = \"runs/part4\"):\n",
        "    if which == 'tensorboard':\n",
        "        tb = TBLogger(out_dir)\n",
        "        return tb if tb.w is not None else NoopLogger()\n",
        "    if which == 'wandb':\n",
        "        return WBLogger(project='llm-part4')\n",
        "    return NoopLogger()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "uhrTZTkuKlxJ"
      },
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "import torch\n",
        "\n",
        "def top_k_top_p_filtering(logits: torch.Tensor, top_k: int | None = None, top_p: float | None = None):\n",
        "    \"\"\"Filter a distribution of logits using top-k and/or nucleus (top-p) filtering.\n",
        "    - logits: (B, vocab)\n",
        "    Returns filtered logits with -inf for masked entries.\n",
        "    \"\"\"\n",
        "    B, V = logits.shape\n",
        "    filtered = logits.clone()\n",
        "\n",
        "    if top_k is not None and top_k < V:\n",
        "        topk_vals, _ = torch.topk(filtered, top_k, dim=-1)\n",
        "        kth = topk_vals[:, -1].unsqueeze(-1)\n",
        "        filtered[filtered < kth] = float('-inf')\n",
        "\n",
        "    if top_p is not None and 0 < top_p < 1.0:\n",
        "        sorted_logits, sorted_idx = torch.sort(filtered, descending=True, dim=-1)\n",
        "        probs = torch.softmax(sorted_logits, dim=-1)\n",
        "        cumsum = torch.cumsum(probs, dim=-1)\n",
        "        mask = cumsum > top_p\n",
        "        # keep at least 1 token\n",
        "        mask[..., 0] = False\n",
        "        sorted_logits[mask] = float('-inf')\n",
        "        # Scatter back\n",
        "        filtered = torch.full_like(filtered, float('-inf'))\n",
        "        filtered.scatter_(1, sorted_idx, sorted_logits)\n",
        "\n",
        "    return filtered"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "vxpn692YKx7K"
      },
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "import torch\n",
        "import math\n",
        "\n",
        "class RoPECache:\n",
        "    \"\"\"Precompute cos/sin for positions up to max_pos for even head_dim.\"\"\"\n",
        "    def __init__(self, head_dim: int, max_pos: int, base: float = 10000.0, device: torch.device | None = None):\n",
        "        assert head_dim % 2 == 0, \"RoPE head_dim must be even\"\n",
        "        self.head_dim = head_dim\n",
        "        self.base = base\n",
        "        self.device = device\n",
        "        self._build(max_pos)\n",
        "    def get(self, positions: torch.Tensor):\n",
        "        # positions: (T,) or (1,T)\n",
        "        if positions.dim() == 2:\n",
        "            positions = positions[0]\n",
        "        need = int(positions.max().item()) + 1 if positions.numel() > 0 else 1\n",
        "        if need > self.max_pos:\n",
        "            # grow tables\n",
        "            self._build(max(need, int(self.max_pos * 2)))\n",
        "        cos = self.cos[positions]  # (T, D/2)\n",
        "        sin = self.sin[positions]\n",
        "        return cos, sin\n",
        "\n",
        "    def _build(self, max_pos: int):\n",
        "        \"\"\"(Re)build cos/sin tables for a new max_pos.\"\"\"\n",
        "        self.max_pos = max_pos\n",
        "        inv_freq = 1.0 / (10000.0 ** (torch.arange(0, self.head_dim, 2, device=self.device).float() / self.head_dim))\n",
        "        t = torch.arange(max_pos, device=self.device).float()\n",
        "        freqs = torch.outer(t, inv_freq)  # (max_pos, head_dim/2)\n",
        "        self.cos = torch.cos(freqs)\n",
        "        self.sin = torch.sin(freqs)\n",
        "\n",
        "def apply_rope_single(x: torch.Tensor, cos: torch.Tensor, sin: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"Rotate pairs along last dim for RoPE.\n",
        "    x: (B,H,T,D) with D even; cos/sin: (T,D/2)\n",
        "    \"\"\"\n",
        "    assert x.size(-1) % 2 == 0\n",
        "    cos = cos.unsqueeze(0).unsqueeze(0)  # (1,1,T,D/2)\n",
        "    sin = sin.unsqueeze(0).unsqueeze(0)\n",
        "    x1 = x[..., ::2]\n",
        "    x2 = x[..., 1::2]\n",
        "    xr1 = x1 * cos - x2 * sin\n",
        "    xr2 = x1 * sin + x2 * cos\n",
        "    out = torch.empty_like(x)\n",
        "    out[..., ::2] = xr1\n",
        "    out[..., 1::2] = xr2\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "8j7_61JWK0gR"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class RMSNorm(nn.Module):\n",
        "    \"\"\"Root Mean Square Layer Normalization.\n",
        "    y = x * g / rms(x),   rms(x) = sqrt(mean(x^2) + eps)\n",
        "    \"\"\"\n",
        "    def __init__(self, dim: int, eps: float = 1e-8):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.weight = nn.Parameter(torch.ones(dim))\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        rms = x.pow(2).mean(dim=-1, keepdim=True).add(self.eps).sqrt()\n",
        "        return (x / rms) * self.weight"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "1h3_SXhJK3Ih"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class SwiGLU(nn.Module):\n",
        "    \"\"\"SwiGLU FFN: (xW1) ⊗ swish(xW2) W3  with expansion factor `mult`.\n",
        "    \"\"\"\n",
        "    def __init__(self, dim: int, mult: int = 4, dropout: float = 0.0):\n",
        "        super().__init__()\n",
        "        inner = mult * dim\n",
        "        self.w1 = nn.Linear(dim, inner, bias=False)\n",
        "        self.w2 = nn.Linear(dim, inner, bias=False)\n",
        "        self.w3 = nn.Linear(inner, dim, bias=False)\n",
        "        self.act = nn.SiLU()\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "    def forward(self, x):\n",
        "        a = self.w1(x)\n",
        "        b = self.act(self.w2(x))\n",
        "        return self.drop(self.w3(a * b))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "Y_dptOYaK61Y"
      },
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "import torch\n",
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass\n",
        "class KVCache:\n",
        "    k: torch.Tensor  # (B,H,T,D)\n",
        "    v: torch.Tensor  # (B,H,T,D)\n",
        "\n",
        "    @property\n",
        "    def T(self):\n",
        "        return self.k.size(2)\n",
        "\n",
        "class RollingKV:\n",
        "    \"\"\"Rolling buffer with optional attention sink.\n",
        "    Keeps first `sink` tokens + last `window` tokens.\n",
        "    \"\"\"\n",
        "    def __init__(self, window: int, sink: int = 0):\n",
        "        self.window = window\n",
        "        self.sink = sink\n",
        "        self.k = None\n",
        "        self.v = None\n",
        "    def step(self, k_new: torch.Tensor, v_new: torch.Tensor):\n",
        "        if self.k is None:\n",
        "            self.k, self.v = k_new, v_new\n",
        "        else:\n",
        "            self.k = torch.cat([self.k, k_new], dim=2)\n",
        "            self.v = torch.cat([self.v, v_new], dim=2)\n",
        "        # crop\n",
        "        if self.k.size(2) > self.window + self.sink:\n",
        "            sink_part = self.k[:, :, :self.sink, :]\n",
        "            sink_val  = self.v[:, :, :self.sink, :]\n",
        "            tail_k = self.k[:, :, -self.window:, :]\n",
        "            tail_v = self.v[:, :, -self.window:, :]\n",
        "            self.k = torch.cat([sink_part, tail_k], dim=2)\n",
        "            self.v = torch.cat([sink_val, tail_v], dim=2)\n",
        "        return self.k, self.v"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "8Z90roi3K9Sh"
      },
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "import math, torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class CausalSelfAttentionModern(nn.Module):\n",
        "    def __init__(self, n_embd: int, n_head: int, dropout: float = 0.0,\n",
        "                 rope: bool = True, max_pos: int = 4096,\n",
        "                 sliding_window: int | None = None, attention_sink: int = 0,\n",
        "                 n_kv_head: int | None = None):  # ← NEW\n",
        "        super().__init__()\n",
        "        assert n_embd % n_head == 0, \"n_embd must be divisible by n_head\"\n",
        "        self.n_head = n_head\n",
        "        self.n_kv_head = n_kv_head or n_head      # ← NEW (GQA defaults to MHA)\n",
        "        assert self.n_head % self.n_kv_head == 0, \"n_head must be multiple of n_kv_head (GQA grouping)\"\n",
        "        self.group_size = self.n_head // self.n_kv_head\n",
        "        self.d_head = n_embd // n_head\n",
        "\n",
        "        # Separate projections for Q vs K/V (sizes differ under GQA)  ← CHANGED\n",
        "        self.wq  = nn.Linear(n_embd, self.n_head   * self.d_head, bias=False)\n",
        "        self.wk  = nn.Linear(n_embd, self.n_kv_head * self.d_head, bias=False)\n",
        "        self.wv  = nn.Linear(n_embd, self.n_kv_head * self.d_head, bias=False)\n",
        "        self.proj = nn.Linear(n_embd, n_embd, bias=False)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.use_rope = rope\n",
        "        self.rope_cache: RoPECache | None = None\n",
        "        self.max_pos = max_pos\n",
        "        self.sliding_window = sliding_window\n",
        "        self.attention_sink = attention_sink\n",
        "\n",
        "    def _maybe_init_rope(self, device):\n",
        "        if self.use_rope and self.rope_cache is None:\n",
        "            self.rope_cache = RoPECache(self.d_head, self.max_pos, device=device)\n",
        "\n",
        "    def forward(self, x: torch.Tensor, kv_cache: KVCache | None = None, start_pos: int = 0):\n",
        "        \"\"\"x: (B,T,C). If kv_cache given, we assume generation (T small, often 1).\"\"\"\n",
        "        B, T, C = x.shape\n",
        "        self._maybe_init_rope(x.device)\n",
        "\n",
        "        # Projections\n",
        "        q = self.wq(x).view(B, T, self.n_head,   self.d_head).transpose(1, 2)    # (B,H, T,D)\n",
        "        k = self.wk(x).view(B, T, self.n_kv_head, self.d_head).transpose(1, 2)   # (B,Hk,T,D)\n",
        "        v = self.wv(x).view(B, T, self.n_kv_head, self.d_head).transpose(1, 2)   # (B,Hk,T,D)\n",
        "\n",
        "        # RoPE on *current* tokens (cached keys are already rotated)\n",
        "        if self.use_rope:\n",
        "            pos = torch.arange(start_pos, start_pos + T, device=x.device)\n",
        "            cos, sin = self.rope_cache.get(pos)\n",
        "            q = apply_rope_single(q, cos, sin)   # (B,H, T,D)\n",
        "            k = apply_rope_single(k, cos, sin)   # (B,Hk,T,D)\n",
        "\n",
        "        # Concatenate past cache (cache is stored in Hk heads)\n",
        "        if kv_cache is not None:\n",
        "            k_all = torch.cat([kv_cache.k, k], dim=2)  # (B,Hk, Tpast+T, D)\n",
        "            v_all = torch.cat([kv_cache.v, v], dim=2)\n",
        "        else:\n",
        "            k_all, v_all = k, v\n",
        "\n",
        "        # Sliding-window + attention-sink (crop along seq length)\n",
        "        if self.sliding_window is not None and k_all.size(2) > (self.sliding_window + self.attention_sink):\n",
        "            s = self.attention_sink\n",
        "            k_all = torch.cat([k_all[:, :, :s, :], k_all[:, :, -self.sliding_window:, :]], dim=2)\n",
        "            v_all = torch.cat([v_all[:, :, :s, :], v_all[:, :, -self.sliding_window:, :]], dim=2)\n",
        "\n",
        "        # --- GQA expand: repeat K/V heads to match Q heads before attention ---\n",
        "        if self.n_kv_head != self.n_head:\n",
        "            k_attn = k_all.repeat_interleave(self.group_size, dim=1)  # (B,H,Tk,D)\n",
        "            v_attn = v_all.repeat_interleave(self.group_size, dim=1)  # (B,H,Tk,D)\n",
        "        else:\n",
        "            k_attn, v_attn = k_all, v_all\n",
        "\n",
        "        # Scaled dot-product attention (PyTorch scales internally)\n",
        "        is_causal = kv_cache is None\n",
        "        y = F.scaled_dot_product_attention(q, k_attn, v_attn,\n",
        "                                           attn_mask=None,\n",
        "                                           dropout_p=self.dropout.p if self.training else 0.0,\n",
        "                                           is_causal=is_causal)          # (B,H,T,D)\n",
        "\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
        "        y = self.proj(y)\n",
        "\n",
        "        # Update KV cache (store compact Hk heads, not expanded)\n",
        "        if kv_cache is not None:\n",
        "            k_new = torch.cat([kv_cache.k, k], dim=2)  # (B,Hk,*,D)\n",
        "            v_new = torch.cat([kv_cache.v, v], dim=2)\n",
        "        else:\n",
        "            k_new, v_new = k, v\n",
        "        new_cache = KVCache(k_new, v_new)\n",
        "        return y, new_cache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "OgL9mN_5K_YY"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "# from rmsnorm import RMSNorm\n",
        "# from swiglu import SwiGLU\n",
        "# from attn_modern import CausalSelfAttentionModern\n",
        "\n",
        "class TransformerBlockModern(nn.Module):\n",
        "    def __init__(self, n_embd: int, n_head: int, dropout: float = 0.0,\n",
        "                 use_rmsnorm: bool = True, use_swiglu: bool = True,\n",
        "                 rope: bool = True, max_pos: int = 4096,\n",
        "                 sliding_window: int | None = None, attention_sink: int = 0, n_kv_head: int | None = None):\n",
        "        super().__init__()\n",
        "        Norm = RMSNorm if use_rmsnorm else nn.LayerNorm\n",
        "        self.ln1 = Norm(n_embd)\n",
        "        self.attn = CausalSelfAttentionModern(n_embd, n_head, dropout, rope, max_pos, sliding_window, attention_sink, n_kv_head)\n",
        "        self.ln2 = Norm(n_embd)\n",
        "        self.ffn = SwiGLU(n_embd, mult=4, dropout=dropout) if use_swiglu else nn.Sequential(\n",
        "            nn.Linear(n_embd, 4*n_embd), nn.GELU(), nn.Linear(4*n_embd, n_embd), nn.Dropout(dropout)\n",
        "        )\n",
        "    def forward(self, x, kv_cache=None, start_pos: int = 0):\n",
        "        a, kv_cache = self.attn(self.ln1(x), kv_cache=kv_cache, start_pos=start_pos)\n",
        "        x = x + a\n",
        "        x = x + self.ffn(self.ln2(x))\n",
        "        return x, kv_cache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "z6aLPG8MLGda"
      },
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "# from block_modern import TransformerBlockModern\n",
        "# from tokenizer import ByteTokenizer\n",
        "\n",
        "# Get the absolute path to the folder that contains part_2 and part_3\n",
        "import os, sys\n",
        "# parent_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))\n",
        "# sys.path.insert(0, parent_dir)\n",
        "\n",
        "class GPTModern(nn.Module):\n",
        "    def __init__(self, vocab_size: int = 256, block_size: int = 256,\n",
        "                 n_layer: int=4, n_head: int=4, n_embd: int=256, dropout: float=0.0,\n",
        "                 use_rmsnorm: bool = True, use_swiglu: bool = True, rope: bool = True,\n",
        "                 max_pos: int = 4096, sliding_window: int | None = None, attention_sink: int = 0, n_kv_head: int | None = None):\n",
        "        super().__init__()\n",
        "        self.block_size = block_size\n",
        "        self.tok_emb = nn.Embedding(vocab_size, n_embd)\n",
        "        # self.pos_emb = nn.Embedding(block_size, n_embd)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.blocks = nn.ModuleList([\n",
        "            TransformerBlockModern(n_embd, n_head, dropout, use_rmsnorm, use_swiglu, rope, max_pos, sliding_window, attention_sink, n_kv_head)\n",
        "            for _ in range(n_layer)\n",
        "        ])\n",
        "        self.ln_f = nn.Identity() if use_rmsnorm else nn.LayerNorm(n_embd)\n",
        "        self.head = nn.Linear(n_embd, vocab_size, bias=False)\n",
        "\n",
        "    def forward(self, idx: torch.Tensor, targets: torch.Tensor | None = None, kv_cache_list=None, start_pos: int = 0):\n",
        "        B, T = idx.shape\n",
        "        assert T <= self.block_size\n",
        "        pos = torch.arange(0, T, device=idx.device).unsqueeze(0)\n",
        "        x = self.tok_emb(idx)\n",
        "        # + self.pos_emb(pos)\n",
        "        x = self.drop(x)\n",
        "\n",
        "        new_caches = []\n",
        "        for i, blk in enumerate(self.blocks):\n",
        "            cache = None if kv_cache_list is None else kv_cache_list[i]\n",
        "            x, cache = blk(x, kv_cache=cache, start_pos=start_pos)\n",
        "            new_caches.append(cache)\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.head(x)\n",
        "\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            import torch.nn.functional as F\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
        "        return logits, loss, new_caches\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self,\n",
        "                 prompt: torch.Tensor,\n",
        "                 max_new_tokens=200,\n",
        "                 temperature=1.0,\n",
        "                 top_k=50,\n",
        "                 top_p=None,\n",
        "                 eos_id=1, # addition from part 6 for early stopping\n",
        "                 sliding_window: int | None = None,\n",
        "                 attention_sink: int = 0):\n",
        "        # try:\n",
        "        #     from utils import top_k_top_p_filtering as _tk'\n",
        "        # except Exception:\n",
        "        #     _tk = lambda x, **_: x\n",
        "\n",
        "        self.eval()\n",
        "        idx = prompt\n",
        "        kvs = [None] * len(self.blocks)\n",
        "\n",
        "        for _ in range(max_new_tokens):\n",
        "            # feed full prompt once; then only the last token\n",
        "            idx_cond = idx[:, -self.block_size:] if kvs[0] is None else idx[:, -1:]\n",
        "\n",
        "            # absolute start position from cache length (0 on first step)\n",
        "            start_pos = 0 if kvs[0] is None else kvs[0].k.size(2)\n",
        "\n",
        "            logits, _, kvs = self(idx_cond, kv_cache_list=kvs, start_pos=start_pos)\n",
        "\n",
        "            next_logits = logits[:, -1, :] / max(temperature, 1e-6)\n",
        "            next_logits = top_k_top_p_filtering(next_logits, top_k=top_k, top_p=top_p)\n",
        "            probs = torch.softmax(next_logits, dim=-1)\n",
        "            next_id = torch.argmax(probs, dim=-1, keepdim=True) if temperature == 0.0 else torch.multinomial(probs, 1)\n",
        "            idx = torch.cat([idx, next_id], dim=1)\n",
        "\n",
        "            # addition from part 6 for early stopping\n",
        "            if eos_id is not None:\n",
        "                if (next_id == eos_id).all():\n",
        "                    break\n",
        "\n",
        "        return idx\n",
        "\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate_nocache(self, prompt: torch.Tensor, max_new_tokens=200, temperature=1.0, top_k=50, top_p=None,\n",
        "                sliding_window: int | None = None, attention_sink: int = 0):\n",
        "        # try:\n",
        "        #     print('from utils import top_k_top_p_filtering as _tk')\n",
        "        # except Exception:\n",
        "        #     _tk = lambda x, **_: x\n",
        "\n",
        "        self.eval()\n",
        "        idx = prompt\n",
        "\n",
        "        for _ in range(max_new_tokens):\n",
        "            # always run a full forward over the cropped window, with NO cache\n",
        "            idx_cond = idx[:, -self.block_size:]\n",
        "            # absolute position of first token in the window (matches cached path)\n",
        "            start_pos = idx.size(1) - idx_cond.size(1)\n",
        "\n",
        "            logits, _, _ = self(idx_cond, kv_cache_list=None, start_pos=start_pos)\n",
        "\n",
        "            next_logits = logits[:, -1, :] / max(temperature, 1e-6)\n",
        "            next_logits = top_k_top_p_filtering(next_logits, top_k=top_k, top_p=top_p)\n",
        "            probs = torch.softmax(next_logits, dim=-1)\n",
        "            topv, topi = torch.topk(probs, 10)\n",
        "            print(\"top ids:\", topi.tolist())\n",
        "            print(\"top vs:\", topv.tolist())\n",
        "            next_id = torch.argmax(probs, dim=-1, keepdim=True) if temperature == 0.0 else torch.multinomial(probs, 1)\n",
        "            idx = torch.cat([idx, next_id], dim=1)\n",
        "\n",
        "        return idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EHfRy2GCJJET",
        "outputId": "b425f8e7-9ad9-4d45-a4d9-b43cde0d9f24"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[init] Trained tokenizer to runs/part4-demo/tokenizer (vocab=1000)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-1027849392.py:14: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self.scaler = torch.cuda.amp.GradScaler(enabled=self.amp)\n",
            "/tmp/ipython-input-1748937790.py:31: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  assert T <= self.block_size\n",
            "/tmp/ipython-input-3837243272.py:17: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  need = int(positions.max().item()) + 1 if positions.numel() > 0 else 1\n",
            "/tmp/ipython-input-3837243272.py:17: TracerWarning: Converting a tensor to a Python number might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  need = int(positions.max().item()) + 1 if positions.numel() > 0 else 1\n",
            "/tmp/ipython-input-3837243272.py:38: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  assert x.size(-1) % 2 == 0\n",
            "/tmp/ipython-input-449061659.py:184: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=amp.amp):\n",
            "/tmp/ipython-input-3920585331.py:85: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.no_grad(), torch.cuda.amp.autocast(enabled=False):\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved checkpoint to runs/part4-demo/model_last.pt\n"
          ]
        }
      ],
      "source": [
        "from __future__ import annotations\n",
        "import argparse, time, signal\n",
        "from pathlib import Path\n",
        "import sys\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# so we can import Part 3 model\n",
        "from pathlib import Path as _P\n",
        "# sys.path.append(str(_P(__file__).resolve().parents[1] / 'part_3'))\n",
        "# # from model_modern import GPTModern\n",
        "\n",
        "# from tokenizer_bpe import BPETokenizer\n",
        "# from dataset_bpe import make_loader\n",
        "# from lr_scheduler import WarmupCosineLR\n",
        "# from amp_accum import AmpGrad\n",
        "# from checkpointing import (\n",
        "#     load_checkpoint,\n",
        "#     _log_hparams_tb,\n",
        "#     _maybe_log_graph_tb,\n",
        "#     _is_tb,\n",
        "#     _log_model_stats,\n",
        "#     _maybe_log_attention,\n",
        "#     _log_samples_tb,\n",
        "#     _log_runtime,\n",
        "#     atomic_save_all,\n",
        "# )\n",
        "# from logger import init_logger\n",
        "\n",
        "\n",
        "def run_cfg_from_args(args, vocab_size: int) -> dict:\n",
        "    return dict(\n",
        "        vocab_size=vocab_size,\n",
        "        block_size=args.block_size,\n",
        "        n_layer=args.n_layer,\n",
        "        n_head=args.n_head,\n",
        "        n_embd=args.n_embd,\n",
        "        dropout=args.dropout,\n",
        "        use_rmsnorm=True,\n",
        "        use_swiglu=True,\n",
        "        rope=True,\n",
        "        max_pos=4096,\n",
        "        sliding_window=None,\n",
        "        attention_sink=0,\n",
        "    )\n",
        "\n",
        "\n",
        "def main(argv:None):\n",
        "    if argv is None:\n",
        "        argv = sys.argv[1:]\n",
        "    p = argparse.ArgumentParser()\n",
        "    p.add_argument('--data', type=str, required=True)\n",
        "    p.add_argument('--out', type=str, default='runs/part4')\n",
        "\n",
        "    # tokenizer / model dims\n",
        "    p.add_argument('--bpe', action='store_true', help='train and use a BPE tokenizer (recommended)')\n",
        "    p.add_argument('--vocab_size', type=int, default=32000)\n",
        "    p.add_argument('--block_size', type=int, default=256)\n",
        "    p.add_argument('--n_layer', type=int, default=6)\n",
        "    p.add_argument('--n_head', type=int, default=8)\n",
        "    p.add_argument('--n_embd', type=int, default=512)\n",
        "    p.add_argument('--dropout', type=float, default=0.0)\n",
        "\n",
        "    # train\n",
        "    p.add_argument('--batch_size', type=int, default=32)\n",
        "    p.add_argument('--epochs', type=int, default=1)\n",
        "    p.add_argument('--steps', type=int, default=300, help='max optimizer steps for this run')\n",
        "    p.add_argument('--lr', type=float, default=3e-4)\n",
        "    p.add_argument('--warmup_steps', type=int, default=20)\n",
        "    p.add_argument('--mixed_precision', action='store_true')\n",
        "    p.add_argument('--grad_accum_steps', type=int, default=4)\n",
        "\n",
        "    # misc\n",
        "    p.add_argument('--log', choices=['wandb', 'tensorboard', 'none'], default='tensorboard')\n",
        "    p.add_argument('--save_every', type=int, default=50, help='save checkpoint every N optimizer steps')\n",
        "    p.add_argument('--keep_last_k', type=int, default=2, help='keep last K step checkpoints (plus model_last.pt)')\n",
        "    args = p.parse_args(argv)\n",
        "\n",
        "    # device\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    # output dir and (possible) checkpoint\n",
        "    out_dir = Path(args.out); out_dir.mkdir(parents=True, exist_ok=True)\n",
        "    ckpt_path = out_dir / \"model_last.pt\"\n",
        "    have_ckpt = ckpt_path.exists()\n",
        "\n",
        "    # ---- load checkpoint meta if present ----\n",
        "    ckpt = None\n",
        "    saved_tok_dir = None\n",
        "    if have_ckpt:\n",
        "        ckpt = torch.load(str(ckpt_path), map_location=device)\n",
        "        if \"config\" not in ckpt:\n",
        "            raise RuntimeError(\n",
        "                \"Checkpoint is missing 'config'.\"\n",
        "                \"Please re-save a checkpoint that includes the model config.\"\n",
        "            )\n",
        "        tok_file = ckpt_path.with_name(\"tokenizer_dir.txt\")\n",
        "        saved_tok_dir = tok_file.read_text().strip() if tok_file.exists() else None\n",
        "\n",
        "    # ---- tokenizer ----\n",
        "    tok = None\n",
        "    tok_dir = None\n",
        "    if have_ckpt:\n",
        "        if not saved_tok_dir:\n",
        "            raise RuntimeError(\n",
        "                \"Checkpoint was found but tokenizer_dir.txt is missing. \"\n",
        "                \"Resume requires the original tokenizer.\"\n",
        "            )\n",
        "        tok = BPETokenizer(); tok.load(saved_tok_dir)\n",
        "        tok_dir = saved_tok_dir\n",
        "        vocab_size = tok.vocab_size\n",
        "        print(f\"[resume] Loaded tokenizer from {tok_dir} (vocab={vocab_size})\")\n",
        "    else:\n",
        "        if args.bpe:\n",
        "            tok = BPETokenizer(vocab_size=args.vocab_size)\n",
        "            tok.train(args.data)\n",
        "            tok_dir = str(out_dir / 'tokenizer'); Path(tok_dir).mkdir(parents=True, exist_ok=True)\n",
        "            tok.save(tok_dir)\n",
        "            vocab_size = tok.vocab_size\n",
        "            print(f\"[init] Trained tokenizer to {tok_dir} (vocab={vocab_size})\")\n",
        "        else:\n",
        "            tok = None\n",
        "            vocab_size = 256  # byte-level fallback (not recommended for Part 4)\n",
        "\n",
        "    # ---- dataset ----\n",
        "    train_loader = make_loader(args.data, tok, args.block_size, args.batch_size, shuffle=True)\n",
        "    try:\n",
        "        print(f\"Number of batches in train_loader: {len(train_loader)}\")\n",
        "        if hasattr(train_loader, \"dataset\"):\n",
        "            print(f\"Number of samples in dataset: {len(train_loader.dataset)}\")\n",
        "    except TypeError:\n",
        "        print(\"train_loader has no __len__ (likely an iterable or streaming dataset).\")\n",
        "\n",
        "\n",
        "    # ---- build model config ----\n",
        "    if have_ckpt:\n",
        "        cfg_build = ckpt[\"config\"]\n",
        "        if cfg_build.get(\"vocab_size\") != vocab_size:\n",
        "            raise RuntimeError(\n",
        "                f\"Tokenizer vocab ({vocab_size}) != checkpoint config vocab ({cfg_build.get('vocab_size')}). \"\n",
        "                \"This deterministic script forbids vocab changes on resume.\"\n",
        "            )\n",
        "    else:\n",
        "        cfg_build = run_cfg_from_args(args, vocab_size)\n",
        "\n",
        "    # ---- init model/opt/sched/amp ----\n",
        "    model = GPTModern(**cfg_build).to(device)\n",
        "    optim = torch.optim.AdamW(model.parameters(), lr=args.lr, betas=(0.9, 0.95), weight_decay=0.1)\n",
        "\n",
        "    total_steps = min(args.steps, args.epochs * len(train_loader))\n",
        "    warmup = min(args.warmup_steps, max(total_steps // 10, 1))\n",
        "    sched = WarmupCosineLR(optim, warmup_steps=warmup, total_steps=total_steps, base_lr=args.lr)\n",
        "\n",
        "    amp = AmpGrad(optim, accum=args.grad_accum_steps, amp=args.mixed_precision)\n",
        "\n",
        "    # ---- strict resume ----\n",
        "    step = 0\n",
        "    if have_ckpt:\n",
        "        step = load_checkpoint(model, str(ckpt_path), optimizer=optim, scheduler=sched, amp=amp, strict=True)\n",
        "        print(f\"[resume] Loaded checkpoint at step {step}\")\n",
        "\n",
        "    # ---- logging ----\n",
        "    logger = init_logger(args.log, out_dir=str(out_dir))\n",
        "    _log_hparams_tb(logger, args, total_steps)\n",
        "    if _is_tb(logger):\n",
        "        try:\n",
        "            ex_x, ex_y = next(iter(train_loader))\n",
        "            _maybe_log_graph_tb(logger, model, ex_x.to(device), ex_y.to(device))\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    # ---- graceful save on SIGINT/SIGTERM ----\n",
        "    save_requested = {\"flag\": False}\n",
        "    def _on_term(sig, frame): save_requested[\"flag\"] = True\n",
        "    signal.signal(signal.SIGTERM, _on_term)\n",
        "    signal.signal(signal.SIGINT,  _on_term)\n",
        "\n",
        "    # ---- train loop ----\n",
        "    model.train()\n",
        "    while step < args.steps:\n",
        "        for xb, yb in train_loader:\n",
        "            if step >= args.steps: break\n",
        "            if save_requested[\"flag\"]:\n",
        "                atomic_save_all(model, optim, sched, amp, step, out_dir, tok_dir, args.keep_last_k, cfg_build)\n",
        "                print(f\"[signal] Saved checkpoint at step {step} to {out_dir}. Exiting.\")\n",
        "                return\n",
        "\n",
        "            it_t0 = time.time()\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            with torch.cuda.amp.autocast(enabled=amp.amp):\n",
        "                logits, loss, _ = model(xb, yb)\n",
        "            amp.backward(loss)\n",
        "\n",
        "            if amp.should_step():\n",
        "                amp.step(); amp.zero_grad()\n",
        "                lr = sched.step()\n",
        "                step += 1\n",
        "\n",
        "                # periodic checkpoint\n",
        "                if step % args.save_every == 0:\n",
        "                    atomic_save_all(model, optim, sched, amp, step, out_dir, tok_dir, args.keep_last_k, cfg_build)\n",
        "                    if _is_tb(logger):\n",
        "                        logger.text(\"meta/checkpoint\", f\"Saved at step {step}\", step)\n",
        "\n",
        "                # logging\n",
        "                if step % 50 == 0:\n",
        "                    train_loss = float(loss.item())\n",
        "                    print(f\"[step {step}] train_loss={train_loss:.4f}, lr={lr:.6f}\")\n",
        "\n",
        "                 \n",
        "                    logger.log(step=step, loss=float(loss.item()), lr=float(lr))\n",
        "                    _log_runtime(logger, step, it_t0, xb, device)\n",
        "                    _log_model_stats(logger, model, step, do_hists=False)\n",
        "                    _maybe_log_attention(logger, model, xb, step, every=100)\n",
        "                    _log_samples_tb(logger, model, tok, xb, device, step, max_new_tokens=64)\n",
        "\n",
        "    # ---- final save ----\n",
        "    atomic_save_all(model, optim, sched, amp, step, out_dir, tok_dir, args.keep_last_k, cfg_build)\n",
        "    print(f\"Saved checkpoint to {out_dir}/model_last.pt\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main([\n",
        "        '--data', '/content/tiny_hi.txt',\n",
        "        '--out', 'runs/part4-demo',\n",
        "        '--bpe',\n",
        "        '--vocab_size', '1000',\n",
        "        '--epochs', '1',\n",
        "        '--steps', '300',\n",
        "        '--batch_size', '16',\n",
        "        '--block_size', '128',\n",
        "        '--n_layer', '2',\n",
        "        '--n_head', '2',\n",
        "        '--n_embd', '128',\n",
        "        '--mixed_precision',\n",
        "        '--grad_accum_steps', '2',\n",
        "        '--log', 'tensorboard'\n",
        "    ])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tm3wemHpNf_O",
        "outputId": "a0c6404b-715a-4920-9a3f-40fcfb7b5f79"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.12/dist-packages (2.19.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (1.75.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (3.9)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorboard) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (5.29.5)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (75.2.0)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (1.17.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (3.1.3)\n",
            "Requirement already satisfied: typing-extensions~=4.12 in /usr/local/lib/python3.12/dist-packages (from grpcio>=1.48.2->tensorboard) (4.15.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard) (3.0.3)\n"
          ]
        }
      ],
      "source": [
        "# !pip install tensorboard\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "U2UVVIdZORSO"
      },
      "outputs": [],
      "source": [
        "# %load_ext tensorboard\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "YOHjUV6_OTZ9"
      },
      "outputs": [],
      "source": [
        "# %tensorboard --logdir runs/part4-demo\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b4h-nHsdOWWU",
        "outputId": "61078b7b-e4fe-4877-deb7-c90af087355a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "जो सुमिरत्रगट तीनान बि न\n"
          ]
        }
      ],
      "source": [
        "from __future__ import annotations\n",
        "import argparse, torch\n",
        "from pathlib import Path\n",
        "\n",
        "# load Part 3 model\n",
        "import sys\n",
        "from pathlib import Path as _P\n",
        "# sys.path.append(str(_P(__file__).resolve().parents[1]/'part_3'))\n",
        "# from model_modern import GPTModern  # noqa: E402\n",
        "\n",
        "# from tokenizer_bpe import BPETokenizer\n",
        "\n",
        "\n",
        "def main(argv=None):\n",
        "    if argv is None:\n",
        "      argv = sys.argv[1:]\n",
        "    p = argparse.ArgumentParser()\n",
        "    p.add_argument('--ckpt', type=str, required=True)\n",
        "    p.add_argument('--prompt', type=str, default='')\n",
        "    p.add_argument('--tokens', type=int, default=100)\n",
        "    p.add_argument('--cpu', action='store_true')\n",
        "    args = p.parse_args(argv)\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() and not args.cpu else 'cpu')\n",
        "\n",
        "    ckpt = torch.load(args.ckpt, map_location='cpu')  # load on CPU first; move model later\n",
        "    sd = ckpt['model']\n",
        "    cfg = ckpt.get('config') or {}\n",
        "\n",
        "    # tokenizer (if present)\n",
        "    tok = None\n",
        "    tok_dir_file = Path(args.ckpt).with_name('tokenizer_dir.txt')\n",
        "    if tok_dir_file.exists():\n",
        "        tok_dir = tok_dir_file.read_text().strip()  # file contains the dir path\n",
        "        tok = BPETokenizer()\n",
        "        tok.load(tok_dir)                            # <-- instance method, pass the directory\n",
        "        vocab_from_tok = tok.vocab_size\n",
        "    else:\n",
        "        vocab_from_tok = None\n",
        "\n",
        "\n",
        "    # ---- build config (prefer saved config; otherwise infer) ----\n",
        "    if not cfg:\n",
        "        # If a tokenizer is present and vocab differs, override with tokenizer vocab\n",
        "        # if vocab_from_tok is not None and cfg.get('vocab_size') != vocab_from_tok:\n",
        "        #     cfg = {**cfg, 'vocab_size': vocab_from_tok}\n",
        "    # else:\n",
        "        # Old checkpoints without config: infer essentials from weights\n",
        "        # tok_emb.weight: [V, C] where C == n_embd\n",
        "        V, C = sd['tok_emb.weight'].shape\n",
        "        # pos_emb.weight: [block_size, C] if present\n",
        "        block_size = sd['pos_emb.weight'].shape[0] if 'pos_emb.weight' in sd else 256\n",
        "        # count transformer blocks present\n",
        "        import re\n",
        "        layer_ids = {int(m.group(1)) for k in sd.keys() if (m := re.match(r\"blocks\\.(\\d+)\\.\", k))}\n",
        "        n_layer = max(layer_ids) + 1 if layer_ids else 1\n",
        "        # pick an n_head that divides C (head count doesn't affect weight shapes)\n",
        "        n_head = 8 if C % 8 == 0 else 4 if C % 4 == 0 else 2 if C % 2 == 0 else 1\n",
        "        cfg = dict(\n",
        "            vocab_size=vocab_from_tok or V,\n",
        "            block_size=block_size,\n",
        "            n_layer=n_layer,\n",
        "            n_head=n_head,\n",
        "            n_embd=C,\n",
        "            dropout=0.0,\n",
        "            use_rmsnorm=True,\n",
        "            use_swiglu=True,\n",
        "            rope=True,\n",
        "            max_pos=4096,\n",
        "            sliding_window=None,\n",
        "            attention_sink=0,\n",
        "        )\n",
        "\n",
        "    # ---- build & load model ----\n",
        "    model = GPTModern(**cfg).to(device).eval()\n",
        "    model.load_state_dict(ckpt['model'])\n",
        "    model.to(device).eval()\n",
        "\n",
        "    # prompt ids\n",
        "    if tok:\n",
        "        ids = tok.encode(args.prompt)\n",
        "        if len(ids) == 0: ids = [10]\n",
        "    else:\n",
        "        ids = [10] if args.prompt == '' else list(args.prompt.encode('utf-8'))\n",
        "    idx = torch.tensor([ids], dtype=torch.long, device=device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        out = model.generate(idx, max_new_tokens=args.tokens)\n",
        "    out_ids = out[0].tolist()\n",
        "    if tok:\n",
        "        print(tok.decode(out_ids))\n",
        "    else:\n",
        "        print(bytes(out_ids).decode('utf-8', errors='ignore'))\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main([\n",
        "        '--ckpt', 'runs/part4-demo/model_last.pt',\n",
        "        '--tokens', '10',\n",
        "        '--prompt', 'जो सुमिरत'\n",
        "    ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bnP6dUADPupU"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
